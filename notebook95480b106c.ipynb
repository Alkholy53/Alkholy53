{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8825323,"sourceType":"datasetVersion","datasetId":5235235},{"sourceId":8860832,"sourceType":"datasetVersion","datasetId":5334664}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nYou can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\nInstructions for setting up Colab are as follows:\n1. Open a new Python 3 notebook.\n2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n4. Run this cell to set up dependencies.\n5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\nNOTE: User is responsible for checking the content of datasets and the applicable licenses and determining if suitable for the intended use.\n\"\"\"\n# Install dependencies\n!pip install wget\n!apt-get install -y sox libsndfile1 ffmpeg\n!pip install text-unidecode\n!pip install matplotlib>=3.3.2\n## Install NeMo\nBRANCH = 'r2.0.0rc0'\n!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH  #egg=nemo_toolkit[all] \n## Grab the config we'll use in this example\n!mkdir configs\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/citrinet/config_bpe.yaml\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/conf/conformer/conformer_ctc_bpe.yaml\n\n\"\"\"\nRemember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\nAlternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\nthat you want to use the \"Run All Cells\" (or similar) option.\n\"\"\"\n# exit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dependances ","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/AmirKaseb/SphinxSpeech.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Alkholy53/ASR-Squad.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install editdistance\n!pip install webdataset\n!pip install pyannote.metrics\n!pip install einops\n! pip install pyannote.core\n! pip install inflect\n! pip install hydra.core\n! pip install lhotse\n!pip install numpy soundfile joblib omegaconf lhotse\n! pip install jiwer\n#pip install -r /kaggle/working/SphinxSpeech/requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1-s-kBiyEabNHIB5AKeSExsC8Xy7uA9TJ ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1hbdyqUKfmInvssLCsWNK1gebcKF5GcZO","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the path to your ZIP file\nzip_file_path = '/kaggle/working/tokenizers_v2.zip'\nunzip_dir = '/kaggle/working/tokenizers_v2/'\n\n# Create the directory to unzip into\nos.makedirs(unzip_dir, exist_ok=True)\n\n# Unzip the file\n!unzip -q {zip_file_path} -d {unzip_dir}\n\n# Check the contents of the directory to ensure the files were unzipped\nos.listdir(unzip_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/configs/conformer_ctc_bpe.yaml\nname: \"Conformer-CTC-BPE\"\n\nmodel:\n  sample_rate: 16000\n  log_prediction: true # enables logging sample predictions in the output during training\n  ctc_reduction: 'mean_batch'\n  skip_nan_grad: false\n\n  train_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_train.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 28 # it is set for LibriSpeech, you may need to update it for your dataset\n    min_duration: 0.384\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/banana.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_test.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  # recommend to SPE Unigram tokenizer with small vocab size of 128 or 256 when using 4x sub-sampling\n  # you may find more detail on how to train a tokenizer at: /scripts/tokenizers/process_asr_text_tokenizer.py\n  tokenizer:\n    dir: \"/kaggle/working/ASR-Squad/Tokenizers/tokenizer_spe_unigram_v64\"  # path to directory which contains either tokenizer.model (bpe) or vocab.txt (wpe)\n    type: bpe  # Can be either bpe (SentencePiece tokenizer) or wpe (WordPiece tokenizer)\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: ${model.sample_rate}\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    log: true\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n    pad_value: 0.0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 2 # set to zero to disable it\n    # you may use lower time_masks for smaller models to have a faster convergence\n    time_masks: 5 # set to zero to disable it\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1 # you may set it if you need different output size other than the default d_model\n    n_layers: 16\n    d_model: 176\n\n    # Sub-sampling params\n    subsampling: striding # vggnet, striding, stacking or stacking_norm, dw_striding\n    subsampling_factor: 4 # must be power of 2 for striding and vggnet\n    subsampling_conv_channels: -1 # -1 sets it to d_model\n    causal_downsampling: false\n\n    # Feed forward module's params\n    ff_expansion_factor: 4\n\n    # Multi-headed Attention Module's params\n    self_attention_model: rel_pos # rel_pos or abs_pos\n    n_heads: 4 # may need to be lower for smaller d_models\n    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention\n    att_context_size: [-1, -1] # -1 means unlimited context\n    att_context_style: regular # regular or chunked_limited\n    xscaling: true # scales up the input embeddings by sqrt(d_model)\n    untie_biases: true # unties the biases of the TransformerXL layers\n    pos_emb_max_len: 5000\n\n    # Convolution module's params\n    conv_kernel_size: 31\n    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)\n    # conv_context_size can be\"causal\" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size\n    # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]\n    conv_context_size: null\n\n    ### regularization\n    dropout: 0.1 # The dropout used in most of the Conformer Modules\n    dropout_pre_encoder: 0.1 # The dropout used before the encoder\n    dropout_emb: 0.0 # The dropout used for embeddings\n    dropout_att: 0.1 # The dropout for multi-headed attention modules\n\n    # set to non-zero to enable stochastic depth\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear  # linear or uniform\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.ConvASRDecoder\n    feat_in: null\n    num_classes: -1\n    vocabulary: []\n\n  # config for InterCTC loss: https://arxiv.org/abs/2102.03216\n  # specify loss weights and which layers to use for InterCTC\n  # e.g., to reproduce the paper results, set loss_weights: [0.3]\n  # and apply_at_layers: [8] (assuming 18 layers). Note that final\n  # layer loss coefficient is automatically adjusted (to 0.7 in above example)\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  optim:\n    name: adamw\n    lr: 5.0\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    # less necessity for weight_decay as we already have large augmentations with SpecAug\n    # you may need weight_decay for large models, stable AMP training, small datasets, or when lower augmentations are used\n    # weight decay of 0.0 with lr of 2.0 also works fine\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: NoamAnnealing\n      d_model: ${model.encoder.d_model}\n      # scheduler config override\n      warmup_steps: 10000\n      warmup_ratio: null\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 100\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 1\n  gradient_clip_val: 0.0\n  precision: 32  # 16, 32, or bf16\n  log_every_n_steps: 10  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: False  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\nexp_manager:\n  exp_dir: \"/kaggle/working/results\"\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 5\n    always_save_nemo: True # saves the checkpoints as nemo files instead of PTL checkpoints\n\n  # you need to set these two to True to continue the training\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n\n  # You may use this section to create a W&B logger\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/configs/conformer_ctc_bpe.yaml\nname: \"Conformer-CTC-BPE\"\n\nmodel:\n  sample_rate: 16000\n  log_prediction: true\n  ctc_reduction: 'mean_batch'\n  skip_nan_grad: false\n\n  train_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_train.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 32  # Increased batch size\n    shuffle: true\n    num_workers: 4  # Reduced to suggested maximum workers\n    pin_memory: true\n    max_duration: 28\n    min_duration: 0.384\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/banana.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 32  # Increased batch size\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 4  # Reduced to suggested maximum workers\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_test.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 32  # Increased batch size\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 4  # Reduced to suggested maximum workers\n    pin_memory: true\n\n  tokenizer:\n    dir: \"/kaggle/working/ASR-Squad/Tokenizers/tokenizer_spe_unigram_v64\"\n    type: bpe\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: ${model.sample_rate}\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    log: true\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n    pad_value: 0.0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 1  # Slight reduction\n    time_masks: 2  # Slight reduction\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1\n    n_layers: 18\n    d_model: 256\n    subsampling: striding\n    subsampling_factor: 4\n    subsampling_conv_channels: -1\n    causal_downsampling: false\n    ff_expansion_factor: 4\n    self_attention_model: rel_pos\n    n_heads: 4\n    att_context_size: [-1, -1]\n    att_context_style: regular\n    xscaling: true\n    untie_biases: true\n    pos_emb_max_len: 5000\n    conv_kernel_size: 31\n    conv_norm_type: 'batch_norm'\n    conv_context_size: null\n    dropout: 0.1\n    dropout_pre_encoder: 0.1\n    dropout_emb: 0.0\n    dropout_att: 0.1\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.ConvASRDecoder\n    feat_in: null\n    num_classes: -1\n    vocabulary: []\n\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  optim:\n    name: adamw\n    lr: 5.0\n    betas: [0.9, 0.98]\n    weight_decay: 1e-3\n\n    sched:\n      name: NoamAnnealing\n      d_model: ${model.encoder.d_model}\n      warmup_steps: 10000\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1\n  num_nodes: 1\n  max_epochs: 20\n  max_steps: -1\n  val_check_interval: 1.0\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 2  # Gradient accumulation\n  gradient_clip_val: 0.0\n  precision: 16  # Mixed precision training\n  log_every_n_steps: 50  # Reduced logging frequency\n  enable_progress_bar: True\n  num_sanity_val_steps: 0\n  check_val_every_n_epoch: 1\n  sync_batchnorm: false  # Disable Sync BatchNorm\n  enable_checkpointing: False\n  logger: false\n  benchmark: false\n\nexp_manager:\n  exp_dir: \"/kaggle/working/results\"\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 1  # Save only the best checkpoint\n    always_save_nemo: True\n\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch speech_to_text_ctc_bpe.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile speech_to_text_ctc_bpe.py\n\n\nimport pytorch_lightning as pl\nfrom omegaconf import OmegaConf\n\nfrom nemo.collections.asr.models.ctc_bpe_models import EncDecCTCModelBPE\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\nfrom nemo.utils.exp_manager import exp_manager\n\n\n@hydra_runner(config_path=\"/kaggle/working/configs/\", config_name=\"conformer_ctc_bpe\")\ndef main(cfg):\n    logging.info(f'Hydra config: {OmegaConf.to_yaml(cfg)}')\n\n    trainer = pl.Trainer(**cfg.trainer)\n    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n    asr_model = EncDecCTCModelBPE(cfg=cfg.model, trainer=trainer)\n\n    # Initialize the weights of the model from another model, if provided via config\n    asr_model.maybe_init_from_pretrained_checkpoint(cfg)\n\n    trainer.fit(asr_model)\n\n    if hasattr(cfg.model, 'test_ds') and cfg.model.test_ds.manifest_filepath is not None:\n        if asr_model.prepare_test(trainer):\n            trainer.test(asr_model)  \n    # Save the model\n    final_model_path = '/kaggle/working/final_asr_model.nemo'\n    asr_model.save_to(final_model_path)\n    logging.info(f'Model saved at {final_model_path}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/speech_to_text_ctc_bpe.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BRANCH = 'r2.0.0rc0'\n\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/transcribe_speech.py\n!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/speech_to_text_eval.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/configs/transcribe_speech.py \\\n  model_path=\"/kaggle/working/SphinxSpeech/Model/first_model.nemo\" \\\n  dataset_manifest=\"/kaggle/input/dataset-ja/final_test.json\" \\\n  output_filename=\"/kaggle/working/test_with_predictions.json\" \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python /kaggle/working/configs/transcribe_speech.py \\\n  #model_path=\"/kaggle/working/final_asr_model.nemo\" \\\n  #dataset_manifest=\"/kaggle/input/dataset-ja/final_test.json\" \\\n  #output_filename=\"/kaggle/working/test_with_predictions.json\" \\\n  #batch_size=8 \\\n # cuda=1 \\\n  #amp=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate WER\n!python /kaggle/working/configs/speech_to_text_eval.py \\\n  dataset_manifest=\"/kaggle/working/test_with_predictions.json\" \\\n  use_cer=False \\\n  only_score_manifest=True\n\n# Calculate CER\n!python /kaggle/working/configs/speech_to_text_eval.py \\\n  dataset_manifest=\"/kaggle/working/test_with_predictions.json\" \\\n  use_cer=True \\\n  only_score_manifest=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/NeMo.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/install_beamsearch_decoders.sh\n#!/usr/bin/env bash\n# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Use this script to install KenLM, OpenSeq2Seq decoder, Flashlight decoder\nshopt -s expand_aliases\n\nNEMO_PATH=/kaggle/working/NeMo  # Path to NeMo folder: /workspace/nemo if you use NeMo/Dockerfile\nif [ \"$#\" -eq 1 ]; then\n  NEMO_PATH=$1\nfi\nKENLM_MAX_ORDER=10 # Maximum order of KenLM model, also specified in the setup_os2s_decoders.py\n\nif [ -d \"$NEMO_PATH\" ]; then\n  echo \"The folder '$NEMO_PATH' exists.\"\nelse\n  echo \"Error: The folder '$NEMO_PATH' does not exist. Specify it as a first command line positional argument!\"\n  exit 1\nfi\ncd $NEMO_PATH\n\nif [ $(id -u) -eq 0 ]; then\n  alias aptupdate='apt-get update'\n  alias b2install='./b2'\nelse\n  alias aptupdate='sudo apt-get update'\n  alias b2install='sudo ./b2'\nfi\n\naptupdate && apt-get upgrade -y && apt-get install -y swig liblzma-dev && rm -rf /var/lib/apt/lists/* # liblzma needed for flashlight decoder\n\n# install Boost package for KenLM\nwget https://boostorg.jfrog.io/artifactory/main/release/1.80.0/source/boost_1_80_0.tar.bz2 --no-check-certificate && tar --bzip2 -xf $NEMO_PATH/boost_1_80_0.tar.bz2 && cd boost_1_80_0 && ./bootstrap.sh && b2install --layout=tagged link=static,shared threading=multi,single install -j4 && cd .. || echo FAILURE\nexport BOOST_ROOT=$NEMO_PATH/boost_1_80_0\n\ngit clone https://github.com/NVIDIA/OpenSeq2Seq\ncd OpenSeq2Seq\ngit checkout ctc-decoders\ncd ..\nmv OpenSeq2Seq/decoders $NEMO_PATH/\nrm -rf OpenSeq2Seq\ncd $NEMO_PATH/decoders\ncp $NEMO_PATH/scripts/installers/setup_os2s_decoders.py ./setup.py\n./setup.sh\n\n# install KenLM\ncd $NEMO_PATH/decoders/kenlm/build && cmake -DKENLM_MAX_ORDER=$KENLM_MAX_ORDER .. && make -j2\ncd $NEMO_PATH/decoders/kenlm\npython setup.py install --max_order=$KENLM_MAX_ORDER\nexport KENLM_LIB=$NEMO_PATH/decoders/kenlm/build/bin\nexport KENLM_ROOT=$NEMO_PATH/decoders/kenlm\ncd ..\n\n# install Flashlight\ngit clone https://github.com/flashlight/text && cd text\npython setup.py bdist_wheel\npip install dist/*.whl\ncd ..\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!./install_beamsearch_decoders.sh","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the old version of CMake\n#!sudo apt-get  remove -y cmake\n\n# Install a newer version of CMake (e.g., from Kitware)\n#!sudo apt-get update -y\n#!sudo apt-get install -y software-properties-common\n#!sudo add-apt-repository ppa:kitware/release\n#!sudo apt-get update -y\n#!sudo apt-get install -y cmake\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/train_kenlm.py\n# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# This script would train an N-gram language model with KenLM library (https://github.com/kpu/kenlm) which can be used\n# with the beam search decoders on top of the ASR models. This script supports both character level and BPE level\n# encodings and models which is detected automatically from the type of the model.\n# After the N-gram model is trained, and stored in the binary format, you may use\n# 'scripts/ngram_lm/eval_beamsearch_ngram.py' to evaluate it on an ASR model.\n#\n# You need to install the KenLM library and also the beam search decoders to use this feature. Please refer\n# to 'scripts/ngram_lm/install_beamsearch_decoders.sh' on how to install them.\n#\n# USAGE: python train_kenlm.py nemo_model_file=<path to the .nemo file of the model> \\\n#                              train_paths=<list of paths to the training text or JSON manifest file> \\\n#                              kenlm_bin_path=<path to the bin folder of KenLM library> \\\n#                              kenlm_model_file=<path to store the binary KenLM model> \\\n#                              ngram_length=<order of N-gram model> \\\n#\n# After training is done, the binary LM model is stored at the path specified by '--kenlm_model_file'.\n# You may find more info on how to use this script at:\n# https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html\n\nimport logging\nimport os\nos.environ['HYDRA_FULL_ERROR'] = '1'\nimport subprocess\nimport sys\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import List\n\nfrom omegaconf import MISSING\n\n# Update the Python path to include the scripts directory\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))\n\nfrom scripts.asr_language_modeling.ngram_lm import kenlm_utils\n\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\n\n\"\"\"\nNeMo's beam search decoders only support char-level encodings. In order to make it work with BPE-level encodings, we\nuse a trick to encode the sub-word tokens of the training data as unicode characters and train a char-level KenLM. \n\"\"\"\n\n\n@dataclass\nclass TrainKenlmConfig:\n    \"\"\"\n    Train an N-gram language model with KenLM to be used with beam search decoder of ASR models.\n    \"\"\"\n\n    train_paths: List[\n        str\n    ] = MISSING  # List of training files or folders. Files can be a plain text file or \".json\" manifest or \".json.gz\". Example: [/path/to/manifest/file,/path/to/folder]\n\n    nemo_model_file: str = MISSING  # The path to '.nemo' file of the ASR model, or name of a pretrained NeMo model\n    kenlm_model_file: str = MISSING  # The path to store the KenLM binary model file\n    ngram_length: int = MISSING  # The order of N-gram LM\n    kenlm_bin_path: str = MISSING  # The path to the bin folder of KenLM.\n\n    preserve_arpa: bool = False  # Whether to preserve the intermediate ARPA file.\n    ngram_prune: List[int] = field(\n        default_factory=lambda: [0]\n    )  # List of digits to prune Ngram. Example: [0,0,1]. See Pruning section on the https://kheafield.com/code/kenlm/estimation\n    cache_path: str = \"\"  # Cache path to save tokenized files.\n    verbose: int = 1  # Verbose level, default is 1.\n\n\n@hydra_runner(config_path=None, config_name='TrainKenlmConfig', schema=TrainKenlmConfig)\ndef main(args: TrainKenlmConfig):\n    train_paths = kenlm_utils.get_train_list(args.train_paths)\n\n    if isinstance(args.ngram_prune, str):\n        args.ngram_prune = [args.ngram_prune]\n\n    tokenizer, encoding_level, is_aggregate_tokenizer = kenlm_utils.setup_tokenizer(args.nemo_model_file)\n\n    if encoding_level == \"subword\":\n        discount_arg = \"--discount_fallback\"  # --discount_fallback is needed for training KenLM for BPE-based models\n    else:\n        discount_arg = \"\"\n\n    arpa_file = f\"{args.kenlm_model_file}.tmp.arpa\"\n    \"\"\" LMPLZ ARGUMENT SETUP \"\"\"\n    kenlm_args = [\n        os.path.join(args.kenlm_bin_path, 'lmplz'),\n        \"-o\",\n        str(args.ngram_length),\n        \"--arpa\",\n        arpa_file,\n        discount_arg,\n        \"--prune\",\n    ] + [str(n) for n in args.ngram_prune]\n\n    if args.cache_path:\n        if not os.path.exists(args.cache_path):\n            os.makedirs(args.cache_path, exist_ok=True)\n\n        \"\"\" DATASET SETUP \"\"\"\n        encoded_train_files = []\n        for file_num, train_file in enumerate(train_paths):\n            logging.info(f\"Encoding the train file '{train_file}' number {file_num+1} out of {len(train_paths)} ...\")\n\n            cached_files = glob(os.path.join(args.cache_path, os.path.split(train_file)[1]) + \"*\")\n            encoded_train_file = os.path.join(args.cache_path, os.path.split(train_file)[1] + f\"_{file_num}.tmp.txt\")\n            if (\n                cached_files and cached_files[0] != encoded_train_file\n            ):  # cached_files exists but has another file name: f\"_{file_num}.tmp.txt\"\n                os.rename(cached_files[0], encoded_train_file)\n                logging.info(\"Rename\", cached_files[0], \"to\", encoded_train_file)\n\n            encoded_train_files.append(encoded_train_file)\n\n        kenlm_utils.iter_files(\n            source_path=train_paths,\n            dest_path=encoded_train_files,\n            tokenizer=tokenizer,\n            encoding_level=encoding_level,\n            is_aggregate_tokenizer=is_aggregate_tokenizer,\n            verbose=args.verbose,\n        )\n\n        first_process_args = [\"cat\"] + encoded_train_files\n        first_process = subprocess.Popen(first_process_args, stdout=subprocess.PIPE, stderr=sys.stderr)\n\n        logging.info(f\"Running lmplz command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n        kenlm_p = subprocess.run(\n            kenlm_args,\n            stdin=first_process.stdout,\n            capture_output=False,\n            text=True,\n            stdout=sys.stdout,\n            stderr=sys.stderr,\n        )\n        first_process.wait()\n\n    else:\n        logging.info(f\"Running lmplz command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n        kenlm_p = subprocess.Popen(kenlm_args, stdout=sys.stdout, stdin=subprocess.PIPE, stderr=sys.stderr)\n\n        kenlm_utils.iter_files(\n            source_path=train_paths,\n            dest_path=kenlm_p.stdin,\n            tokenizer=tokenizer,\n            encoding_level=encoding_level,\n            is_aggregate_tokenizer=is_aggregate_tokenizer,\n            verbose=args.verbose,\n        )\n\n        kenlm_p.communicate()\n\n    if kenlm_p.returncode != 0:\n        raise RuntimeError(\"Training KenLM was not successful!\")\n\n    \"\"\" BINARY BUILD \"\"\"\n\n    kenlm_args = [\n        os.path.join(args.kenlm_bin_path, \"build_binary\"),\n        \"trie\",\n        arpa_file,\n        args.kenlm_model_file,\n    ]\n    logging.info(f\"Running binary_build command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n    ret = subprocess.run(kenlm_args, capture_output=False, text=True, stdout=sys.stdout, stderr=sys.stderr)\n\n    if ret.returncode != 0:\n        raise RuntimeError(\"Training KenLM was not successful!\")\n\n    if not args.preserve_arpa:\n        os.remove(arpa_file)\n        logging.info(f\"Deleted the arpa file '{arpa_file}'.\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/NeMo","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/train_kenlm.py nemo_model_file=\"/kaggle/working/final_asr_model.nemo\" \\\n                          train_paths=\"[\\\"/kaggle/input/dataset-ja/final_train.json\\\"]\" \\\n                          kenlm_bin_path=\"/kaggle/working/NeMo/decoders/kenlm/build/bin\" \\\n                          kenlm_model_file=\"/kaggle/working/kenlm_model.binary\" \\\n                          ngram_length=6 \\\n                          preserve_arpa=true\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['HYDRA_FULL_ERROR'] = '1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\n# This script would evaluate an N-gram language model trained with KenLM library (https://github.com/kpu/kenlm) in\n# fusion with beam search decoders on top of a trained ASR model with CTC decoder. To evaluate a model with \n# Transducer (RNN-T) decoder use another script 'scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py'. \n# NeMo's beam search decoders are capable of using the KenLM's N-gram models\n# to find the best candidates. This script supports both character level and BPE level\n# encodings and models which is detected automatically from the type of the model.\n# You may train the LM model with 'scripts/asr_language_modeling/ngram_lm/train_kenlm.py'.\n\n# Config Help\n\nTo discover all arguments of the script, please run :\npython eval_beamsearch_ngram_ctc.py --help\npython eval_beamsearch_ngram_ctc.py --cfg job\n\n# USAGE\n\npython eval_beamsearch_ngram_ctc.py nemo_model_file=<path to the .nemo file of the model> \\\n           input_manifest=<path to the evaluation JSON manifest file> \\\n           kenlm_model_file=<path to the binary KenLM model> \\\n           beam_width=[<list of the beam widths, separated with commas>] \\\n           beam_alpha=[<list of the beam alphas, separated with commas>] \\\n           beam_beta=[<list of the beam betas, separated with commas>] \\\n           preds_output_folder=<optional folder to store the predictions> \\\n           probs_cache_file=null \\\n           decoding_mode=beamsearch_ngram\n           ...\n\n\n# Grid Search for Hyper parameters\n\nFor grid search, you can provide a list of arguments as follows -\n\n           beam_width=[4,8,16,....] \\\n           beam_alpha=[-2.0,-1.0,...,1.0,2.0] \\\n           beam_beta=[-1.0,-0.5,0.0,...,1.0] \\\n\n# You may find more info on how to use this script at:\n# https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html\n\n\"\"\"\n\n\nimport contextlib\nimport json\nimport os\nimport pickle\nfrom dataclasses import dataclass, field, is_dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport editdistance\nimport numpy as np\nimport torch\nfrom omegaconf import MISSING, OmegaConf\nfrom sklearn.model_selection import ParameterGrid\nfrom tqdm.auto import tqdm\n\nimport nemo.collections.asr as nemo_asr\nfrom nemo.collections.asr.models import EncDecHybridRNNTCTCModel\nfrom nemo.collections.asr.parts.submodules import ctc_beam_decoding\nfrom nemo.collections.asr.parts.utils.transcribe_utils import PunctuationCapitalization, TextProcessingConfig\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\n\n# fmt: off\n\n\n@dataclass\nclass EvalBeamSearchNGramConfig:\n    \"\"\"\n    Evaluate an ASR model with beam search decoding and n-gram KenLM language model.\n    \"\"\"\n    # # The path of the '.nemo' file of the ASR model or the name of a pretrained model (ngc / huggingface)\n    nemo_model_file: str = MISSING\n\n    # File paths\n    input_manifest: str = MISSING  # The manifest file of the evaluation set\n    kenlm_model_file: Optional[str] = None  # The path of the KenLM binary model file\n    preds_output_folder: Optional[str] = None  # The optional folder where the predictions are stored\n    probs_cache_file: Optional[str] = None  # The cache file for storing the logprobs of the model\n\n    # Parameters for inference\n    acoustic_batch_size: int = 16  # The batch size to calculate log probabilities\n    beam_batch_size: int = 128  # The batch size to be used for beam search decoding\n    device: str = \"cuda\"  # The device to load the model onto to calculate log probabilities\n    use_amp: bool = False  # Whether to use AMP if available to calculate log probabilities\n\n    # Beam Search hyperparameters\n\n    # The decoding scheme to be used for evaluation.\n    # Can be one of [\"greedy\", \"beamsearch\", \"beamsearch_ngram\"]\n    decoding_mode: str = \"beamsearch_ngram\"\n\n    beam_width: List[int] = field(default_factory=lambda: [128])  # The width or list of the widths for the beam search decoding\n    beam_alpha: List[float] = field(default_factory=lambda: [1.0])  # The alpha parameter or list of the alphas for the beam search decoding\n    beam_beta: List[float] = field(default_factory=lambda: [0.0])  # The beta parameter or list of the betas for the beam search decoding\n\n    decoding_strategy: str = \"beam\"\n    decoding: ctc_beam_decoding.BeamCTCInferConfig = field(default_factory=lambda: ctc_beam_decoding.BeamCTCInferConfig(beam_size=128))\n    \n    text_processing: Optional[TextProcessingConfig] = field(default_factory=lambda: TextProcessingConfig(\n        punctuation_marks = \".,?\",\n        separate_punctuation = False,\n        do_lowercase = False,\n        rm_punctuation = False,\n    ))\n# fmt: on\n\n\ndef beam_search_eval(\n    model: nemo_asr.models.ASRModel,\n    cfg: EvalBeamSearchNGramConfig,\n    all_probs: List[torch.Tensor],\n    target_transcripts: List[str],\n    preds_output_file: str = None,\n    lm_path: str = None,\n    beam_alpha: float = 1.0,\n    beam_beta: float = 0.0,\n    beam_width: int = 128,\n    beam_batch_size: int = 128,\n    progress_bar: bool = True,\n    punctuation_capitalization: PunctuationCapitalization = None,\n):\n    level = logging.getEffectiveLevel()\n    logging.setLevel(logging.CRITICAL)\n    # Reset config\n    if isinstance(model, EncDecHybridRNNTCTCModel):\n        model.change_decoding_strategy(decoding_cfg=None, decoder_type=\"ctc\")\n    else:\n        model.change_decoding_strategy(None)\n\n    # Override the beam search config with current search candidate configuration\n    cfg.decoding.beam_size = beam_width\n    cfg.decoding.beam_alpha = beam_alpha\n    cfg.decoding.beam_beta = beam_beta\n    cfg.decoding.return_best_hypothesis = False\n    cfg.decoding.kenlm_path = cfg.kenlm_model_file\n\n    # Update model's decoding strategy config\n    model.cfg.decoding.strategy = cfg.decoding_strategy\n    model.cfg.decoding.beam = cfg.decoding\n\n    # Update model's decoding strategy\n    if isinstance(model, EncDecHybridRNNTCTCModel):\n        model.change_decoding_strategy(model.cfg.decoding, decoder_type='ctc')\n        decoding = model.ctc_decoding\n    else:\n        model.change_decoding_strategy(model.cfg.decoding)\n        decoding = model.decoding\n    logging.setLevel(level)\n\n    wer_dist_first = cer_dist_first = 0\n    wer_dist_best = cer_dist_best = 0\n    words_count = 0\n    chars_count = 0\n    sample_idx = 0\n    if preds_output_file:\n        out_file = open(preds_output_file, 'w', encoding='utf_8', newline='\\n')\n\n    if progress_bar:\n        it = tqdm(\n            range(int(np.ceil(len(all_probs) / beam_batch_size))),\n            desc=f\"Beam search decoding with width={beam_width}, alpha={beam_alpha}, beta={beam_beta}\",\n            ncols=120,\n        )\n    else:\n        it = range(int(np.ceil(len(all_probs) / beam_batch_size)))\n    for batch_idx in it:\n        # disabling type checking\n        probs_batch = all_probs[batch_idx * beam_batch_size : (batch_idx + 1) * beam_batch_size]\n        probs_lens = torch.tensor([prob.shape[0] for prob in probs_batch])\n        with torch.no_grad():\n            packed_batch = torch.zeros(len(probs_batch), max(probs_lens), probs_batch[0].shape[-1], device='cpu')\n\n            for prob_index in range(len(probs_batch)):\n                packed_batch[prob_index, : probs_lens[prob_index], :] = torch.tensor(\n                    probs_batch[prob_index], device=packed_batch.device, dtype=packed_batch.dtype\n                )\n\n            _, beams_batch = decoding.ctc_decoder_predictions_tensor(\n                packed_batch, decoder_lengths=probs_lens, return_hypotheses=True,\n            )\n\n        for beams_idx, beams in enumerate(beams_batch):\n            target = target_transcripts[sample_idx + beams_idx]\n            target_split_w = target.split()\n            target_split_c = list(target)\n            words_count += len(target_split_w)\n            chars_count += len(target_split_c)\n            wer_dist_min = cer_dist_min = 10000\n            for candidate_idx, candidate in enumerate(beams):  # type: (int, ctc_beam_decoding.rnnt_utils.Hypothesis)\n                pred_text = candidate.text\n                if cfg.text_processing.do_lowercase:\n                    pred_text = punctuation_capitalization.do_lowercase([pred_text])[0]\n                if cfg.text_processing.rm_punctuation:\n                    pred_text = punctuation_capitalization.rm_punctuation([pred_text])[0]\n                if cfg.text_processing.separate_punctuation:\n                    pred_text = punctuation_capitalization.separate_punctuation([pred_text])[0]\n                pred_split_w = pred_text.split()\n                wer_dist = editdistance.eval(target_split_w, pred_split_w)\n                pred_split_c = list(pred_text)\n                cer_dist = editdistance.eval(target_split_c, pred_split_c)\n\n                wer_dist_min = min(wer_dist_min, wer_dist)\n                cer_dist_min = min(cer_dist_min, cer_dist)\n\n                if candidate_idx == 0:\n                    # first candidate\n                    wer_dist_first += wer_dist\n                    cer_dist_first += cer_dist\n\n                score = candidate.score\n                if preds_output_file:\n                    out_file.write('{}\\t{}\\n'.format(pred_text, score))\n            wer_dist_best += wer_dist_min\n            cer_dist_best += cer_dist_min\n        sample_idx += len(probs_batch)\n\n    if preds_output_file:\n        out_file.close()\n        logging.info(f\"Stored the predictions of beam search decoding at '{preds_output_file}'.\")\n\n    if lm_path:\n        logging.info(\n            'WER/CER with beam search decoding and N-gram model = {:.2%}/{:.2%}'.format(\n                wer_dist_first / words_count, cer_dist_first / chars_count\n            )\n        )\n    else:\n        logging.info(\n            'WER/CER with beam search decoding = {:.2%}/{:.2%}'.format(\n                wer_dist_first / words_count, cer_dist_first / chars_count\n            )\n        )\n    logging.info(\n        'Oracle WER/CER in candidates with perfect LM= {:.2%}/{:.2%}'.format(\n            wer_dist_best / words_count, cer_dist_best / chars_count\n        )\n    )\n    logging.info(f\"=================================================================================\")\n\n    return wer_dist_first / words_count, cer_dist_first / chars_count\n\n\n@hydra_runner(config_path=None, config_name='EvalBeamSearchNGramConfig', schema=EvalBeamSearchNGramConfig)\ndef main(cfg: EvalBeamSearchNGramConfig):\n    if is_dataclass(cfg):\n        cfg = OmegaConf.structured(cfg)  # type: EvalBeamSearchNGramConfig\n\n    valid_decoding_modes = [\"greedy\", \"beamsearch\", \"beamsearch_ngram\"]\n    if cfg.decoding_mode not in valid_decoding_modes:\n        raise ValueError(\n            f\"Given decoding_mode={cfg.decoding_mode} is invalid. Available options are :\\n\" f\"{valid_decoding_modes}\"\n        )\n\n    if cfg.nemo_model_file.endswith('.nemo'):\n        asr_model = nemo_asr.models.ASRModel.restore_from(cfg.nemo_model_file, map_location=torch.device(cfg.device))\n    else:\n        logging.warning(\n            \"nemo_model_file does not end with .nemo, therefore trying to load a pretrained model with this name.\"\n        )\n        asr_model = nemo_asr.models.ASRModel.from_pretrained(\n            cfg.nemo_model_file, map_location=torch.device(cfg.device)\n        )\n\n    target_transcripts = []\n    manifest_dir = Path(cfg.input_manifest).parent\n    with open(cfg.input_manifest, 'r', encoding='utf_8') as manifest_file:\n        audio_file_paths = []\n        for line in tqdm(manifest_file, desc=f\"Reading Manifest {cfg.input_manifest} ...\", ncols=120):\n            data = json.loads(line)\n            audio_file = Path(data['audio_filepath'])\n            if not audio_file.is_file() and not audio_file.is_absolute():\n                audio_file = manifest_dir / audio_file\n            target_transcripts.append(data['text'])\n            audio_file_paths.append(str(audio_file.absolute()))\n\n    punctuation_capitalization = PunctuationCapitalization(cfg.text_processing.punctuation_marks)\n    if cfg.text_processing.do_lowercase:\n        target_transcripts = punctuation_capitalization.do_lowercase(target_transcripts)\n    if cfg.text_processing.rm_punctuation:\n        target_transcripts = punctuation_capitalization.rm_punctuation(target_transcripts)\n    if cfg.text_processing.separate_punctuation:\n        target_transcripts = punctuation_capitalization.separate_punctuation(target_transcripts)\n\n    if cfg.probs_cache_file and os.path.exists(cfg.probs_cache_file):\n        logging.info(f\"Found a pickle file of probabilities at '{cfg.probs_cache_file}'.\")\n        logging.info(f\"Loading the cached pickle file of probabilities from '{cfg.probs_cache_file}' ...\")\n        with open(cfg.probs_cache_file, 'rb') as probs_file:\n            all_probs = pickle.load(probs_file)\n\n        if len(all_probs) != len(audio_file_paths):\n            raise ValueError(\n                f\"The number of samples in the probabilities file '{cfg.probs_cache_file}' does not \"\n                f\"match the manifest file. You may need to delete the probabilities cached file.\"\n            )\n    else:\n\n        @contextlib.contextmanager\n        def default_autocast():\n            yield\n\n        if cfg.use_amp:\n            if torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):\n                logging.info(\"AMP is enabled!\\n\")\n                autocast = torch.cuda.amp.autocast\n\n            else:\n                autocast = default_autocast\n        else:\n\n            autocast = default_autocast\n\n        with autocast():\n            with torch.no_grad():\n                if isinstance(asr_model, EncDecHybridRNNTCTCModel):\n                    asr_model.cur_decoder = 'ctc'\n                all_logits = asr_model.transcribe(audio_file_paths, batch_size=cfg.acoustic_batch_size)\n\n        all_probs = all_logits\n        if cfg.probs_cache_file:\n            os.makedirs(os.path.split(cfg.probs_cache_file)[0], exist_ok=True)\n            logging.info(f\"Writing pickle files of probabilities at '{cfg.probs_cache_file}'...\")\n            with open(cfg.probs_cache_file, 'wb') as f_dump:\n                pickle.dump(all_probs, f_dump)\n\n    wer_dist_greedy = 0\n    cer_dist_greedy = 0\n    words_count = 0\n    chars_count = 0\n    for batch_idx, probs in enumerate(all_probs):\n        preds = np.argmax(probs, axis=0)\n        preds_tensor = torch.tensor(preds, device='cpu').unsqueeze(0)\n        if isinstance(asr_model, EncDecHybridRNNTCTCModel):\n            pred_text = asr_model.ctc_decoding.ctc_decoder_predictions_tensor(preds_tensor)[0][0]\n        else:\n                # Debugging: Print shapes and lengths\n            print(\"Shape of preds_tensor:\", preds_tensor.shape)\n            print(\"Length of preds_tensor:\", len(preds_tensor))\n\n            # Ensure decoder_lengths is provided\n            decoder_lengths = torch.full(\n                [preds_tensor.shape[0]], preds_tensor.shape[1], dtype=torch.long, device=preds_tensor.device\n            )\n\n            # Debugging: Print decoder lengths\n            print(\"Decoder lengths:\", decoder_lengths)\n            pred_text = asr_model.wer.decoding.ctc_decoder_predictions_tensor(preds_tensor)[0][0]\n\n        if cfg.text_processing.do_lowercase:\n            pred_text = punctuation_capitalization.do_lowercase([pred_text])[0]\n        if cfg.text_processing.rm_punctuation:\n            pred_text = punctuation_capitalization.rm_punctuation([pred_text])[0]\n        if cfg.text_processing.separate_punctuation:\n            pred_text = punctuation_capitalization.separate_punctuation([pred_text])[0]\n\n        pred_split_w = pred_text.split()\n        target_split_w = target_transcripts[batch_idx].split()\n        pred_split_c = list(pred_text)\n        target_split_c = list(target_transcripts[batch_idx])\n\n        wer_dist = editdistance.eval(target_split_w, pred_split_w)\n        cer_dist = editdistance.eval(target_split_c, pred_split_c)\n\n        wer_dist_greedy += wer_dist\n        cer_dist_greedy += cer_dist\n        words_count += len(target_split_w)\n        chars_count += len(target_split_c)\n\n    logging.info('Greedy WER/CER = {:.2%}/{:.2%}'.format(wer_dist_greedy / words_count, cer_dist_greedy / chars_count))\n\n    asr_model = asr_model.to('cpu')\n\n    if cfg.decoding_mode == \"beamsearch_ngram\":\n        if not os.path.exists(cfg.kenlm_model_file):\n            raise FileNotFoundError(f\"Could not find the KenLM model file '{cfg.kenlm_model_file}'.\")\n        lm_path = cfg.kenlm_model_file\n    else:\n        lm_path = None\n\n    # 'greedy' decoding_mode would skip the beam search decoding\n    if cfg.decoding_mode in [\"beamsearch_ngram\", \"beamsearch\"]:\n        if cfg.beam_width is None or cfg.beam_alpha is None or cfg.beam_beta is None:\n            raise ValueError(\"beam_width, beam_alpha and beam_beta are needed to perform beam search decoding.\")\n        params = {'beam_width': cfg.beam_width, 'beam_alpha': cfg.beam_alpha, 'beam_beta': cfg.beam_beta}\n        hp_grid = ParameterGrid(params)\n        hp_grid = list(hp_grid)\n\n        best_wer_beam_size, best_cer_beam_size = None, None\n        best_wer_alpha, best_cer_alpha = None, None\n        best_wer_beta, best_cer_beta = None, None\n        best_wer, best_cer = 1e6, 1e6\n\n        logging.info(f\"==============================Starting the beam search decoding===============================\")\n        logging.info(f\"Grid search size: {len(hp_grid)}\")\n        logging.info(f\"It may take some time...\")\n        logging.info(f\"==============================================================================================\")\n\n        if cfg.preds_output_folder and not os.path.exists(cfg.preds_output_folder):\n            os.mkdir(cfg.preds_output_folder)\n        for hp in hp_grid:\n            if cfg.preds_output_folder:\n                preds_output_file = os.path.join(\n                    cfg.preds_output_folder,\n                    f\"preds_out_width{hp['beam_width']}_alpha{hp['beam_alpha']}_beta{hp['beam_beta']}.tsv\",\n                )\n            else:\n                preds_output_file = None\n\n            candidate_wer, candidate_cer = beam_search_eval(\n                asr_model,\n                cfg,\n                all_probs=all_probs,\n                target_transcripts=target_transcripts,\n                preds_output_file=preds_output_file,\n                lm_path=lm_path,\n                beam_width=hp[\"beam_width\"],\n                beam_alpha=hp[\"beam_alpha\"],\n                beam_beta=hp[\"beam_beta\"],\n                beam_batch_size=cfg.beam_batch_size,\n                progress_bar=True,\n                punctuation_capitalization=punctuation_capitalization,\n            )\n\n            if candidate_cer < best_cer:\n                best_cer_beam_size = hp[\"beam_width\"]\n                best_cer_alpha = hp[\"beam_alpha\"]\n                best_cer_beta = hp[\"beam_beta\"]\n                best_cer = candidate_cer\n\n            if candidate_wer < best_wer:\n                best_wer_beam_size = hp[\"beam_width\"]\n                best_wer_alpha = hp[\"beam_alpha\"]\n                best_wer_beta = hp[\"beam_beta\"]\n                best_wer = candidate_wer\n\n        logging.info(\n            f'Best WER Candidate = {best_wer:.2%} :: Beam size = {best_wer_beam_size}, '\n            f'Beam alpha = {best_wer_alpha}, Beam beta = {best_wer_beta}'\n        )\n\n        logging.info(\n            f'Best CER Candidate = {best_cer:.2%} :: Beam size = {best_cer_beam_size}, '\n            f'Beam alpha = {best_cer_alpha}, Beam beta = {best_cer_beta}'\n        )\n        logging.info(f\"=================================================================================\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the JSON data from the file\nwith open('/kaggle/input/dataset-ja/banana.json', 'r', encoding='utf-8') as file:\n    lines = file.readlines()\n\n# Remove the \"duration\" field from each JSON object\nmodified_lines = []\nfor line in lines:\n    item = json.loads(line)\n    if 'duration' in item:\n        del item['duration']\n    modified_lines.append(json.dumps(item, ensure_ascii=False))\n\n# Save the modified JSON data back to the file\nwith open('output_file.json', 'w', encoding='utf-8') as file:\n    for line in modified_lines:\n        file.write(line + '\\n')\n\nprint(\"The 'duration' field has been removed.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /opt/conda/lib/python3.10/site-packages/nemo/collections/asr/parts/submodules/ctc_greedy_decoding.py\n# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport torch\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom nemo.collections.asr.parts.utils import rnnt_utils\nfrom nemo.collections.asr.parts.utils.asr_confidence_utils import ConfidenceMethodConfig, ConfidenceMethodMixin\nfrom nemo.core.classes import Typing, typecheck\nfrom nemo.core.neural_types import HypothesisType, LengthsType, LogprobsType, NeuralType\nfrom nemo.utils import logging, logging_mode\n\n\ndef pack_hypotheses(\n    hypotheses: List[rnnt_utils.Hypothesis],\n    logitlen: torch.Tensor,\n) -> List[rnnt_utils.Hypothesis]:\n\n    if logitlen is not None:\n        if hasattr(logitlen, 'cpu'):\n            logitlen_cpu = logitlen.to('cpu')\n        else:\n            logitlen_cpu = logitlen\n\n    for idx, hyp in enumerate(hypotheses):  # type: rnnt_utils.Hypothesis\n        hyp.y_sequence = torch.tensor(hyp.y_sequence, dtype=torch.long)\n\n        if logitlen is not None:\n            hyp.length = logitlen_cpu[idx]\n\n        if hyp.dec_state is not None:\n            hyp.dec_state = _states_to_device(hyp.dec_state)\n\n    return hypotheses\n\n\ndef _states_to_device(dec_state, device='cpu'):\n    if torch.is_tensor(dec_state):\n        dec_state = dec_state.to(device)\n\n    elif isinstance(dec_state, (list, tuple)):\n        dec_state = tuple(_states_to_device(dec_i, device) for dec_i in dec_state)\n\n    return dec_state\n\n\n_DECODER_LENGTHS_NONE_WARNING = \"Passing in decoder_lengths=None for CTC decoding is likely to be an error, since it is unlikely that each element of your batch has exactly the same length. decoder_lengths will default to decoder_output.shape[0].\"\n\n\nclass GreedyCTCInfer(Typing, ConfidenceMethodMixin):\n    \"\"\"A greedy CTC decoder.\n\n    Provides a common abstraction for sample level and batch level greedy decoding.\n\n    Args:\n        blank_index: int index of the blank token. Can be 0 or len(vocabulary).\n        preserve_alignments: Bool flag which preserves the history of logprobs generated during\n            decoding (sample / batched). When set to true, the Hypothesis will contain\n            the non-null value for `logprobs` in it. Here, `logprobs` is a torch.Tensors.\n        compute_timestamps: A bool flag, which determines whether to compute the character/subword, or\n                word based timestamp mapping the output log-probabilities to discrite intervals of timestamps.\n                The timestamps will be available in the returned Hypothesis.timestep as a dictionary.\n        preserve_frame_confidence: Bool flag which preserves the history of per-frame confidence scores\n            generated during decoding. When set to true, the Hypothesis will contain\n            the non-null value for `frame_confidence` in it. Here, `frame_confidence` is a List of floats.\n        confidence_method_cfg: A dict-like object which contains the method name and settings to compute per-frame\n            confidence scores.\n\n            name: The method name (str).\n                Supported values:\n                    - 'max_prob' for using the maximum token probability as a confidence.\n                    - 'entropy' for using a normalized entropy of a log-likelihood vector.\n\n            entropy_type: Which type of entropy to use (str). Used if confidence_method_cfg.name is set to `entropy`.\n                Supported values:\n                    - 'gibbs' for the (standard) Gibbs entropy. If the alpha () is provided,\n                        the formula is the following: H_ = -sum_i((p^_i)*log(p^_i)).\n                        Note that for this entropy, the alpha should comply the following inequality:\n                        (log(V)+2-sqrt(log^2(V)+4))/(2*log(V)) <=  <= (1+log(V-1))/log(V-1)\n                        where V is the model vocabulary size.\n                    - 'tsallis' for the Tsallis entropy with the Boltzmann constant one.\n                        Tsallis entropy formula is the following: H_ = 1/(-1)*(1-sum_i(p^_i)),\n                        where  is a parameter. When  == 1, it works like the Gibbs entropy.\n                        More: https://en.wikipedia.org/wiki/Tsallis_entropy\n                    - 'renyi' for the Rnyi entropy.\n                        Rnyi entropy formula is the following: H_ = 1/(1-)*log_2(sum_i(p^_i)),\n                        where  is a parameter. When  == 1, it works like the Gibbs entropy.\n                        More: https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy\n\n            alpha: Power scale for logsoftmax ( for entropies). Here we restrict it to be > 0.\n                When the alpha equals one, scaling is not applied to 'max_prob',\n                and any entropy type behaves like the Shannon entropy: H = -sum_i(p_i*log(p_i))\n\n            entropy_norm: A mapping of the entropy value to the interval [0,1].\n                Supported values:\n                    - 'lin' for using the linear mapping.\n                    - 'exp' for using exponential mapping with linear shift.\n\n    \"\"\"\n\n    @property\n    def input_types(self):\n        \"\"\"Returns definitions of module input ports.\"\"\"\n        # Input can be of dimension -\n        # ('B', 'T', 'D') [Log probs] or ('B', 'T') [Labels]\n\n        return {\n            \"decoder_output\": NeuralType(None, LogprobsType()),\n            \"decoder_lengths\": NeuralType(tuple('B'), LengthsType()),\n        }\n\n    @property\n    def output_types(self):\n        \"\"\"Returns definitions of module output ports.\"\"\"\n        return {\"predictions\": [NeuralType(elements_type=HypothesisType())]}\n\n    def __init__(\n        self,\n        blank_id: int,\n        preserve_alignments: bool = False,\n        compute_timestamps: bool = False,\n        preserve_frame_confidence: bool = False,\n        confidence_method_cfg: Optional[DictConfig] = None,\n    ):\n        super().__init__()\n\n        self.blank_id = blank_id\n        self.preserve_alignments = preserve_alignments\n        # we need timestamps to extract non-blank per-frame confidence\n        self.compute_timestamps = compute_timestamps | preserve_frame_confidence\n        self.preserve_frame_confidence = preserve_frame_confidence\n\n        # set confidence calculation method\n        self._init_confidence_method(confidence_method_cfg)\n\n    @typecheck()\n    def forward(\n        self,\n        decoder_output: torch.Tensor,\n        decoder_lengths: Optional[torch.Tensor],\n    ):\n        \"\"\"Returns a list of hypotheses given an input batch of the encoder hidden embedding.\n        Output token is generated auto-repressively.\n\n        Args:\n            decoder_output: A tensor of size (batch, timesteps, features) or (batch, timesteps) (each timestep is a label).\n            decoder_lengths: list of int representing the length of each sequence\n                output sequence.\n\n        Returns:\n            packed list containing batch number of sentences (Hypotheses).\n        \"\"\"\n\n        logging.warning(\n            \"CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\",\n            mode=logging_mode.ONCE,\n        )\n\n        if decoder_lengths is None:\n            logging.warning(_DECODER_LENGTHS_NONE_WARNING, mode=logging_mode.ONCE)\n\n        with torch.inference_mode():\n            hypotheses = []\n            # Process each sequence independently\n\n            if decoder_output.is_cuda:\n                # This two-liner is around twenty times faster than:\n                # `prediction_cpu_tensor = decoder_output.cpu()`\n                # cpu() does not use pinned memory, meaning that a slow pageable\n                # copy must be done instead.\n                prediction_cpu_tensor = torch.empty(\n                    decoder_output.shape, dtype=decoder_output.dtype, device=torch.device(\"cpu\"), pin_memory=True\n                )\n                prediction_cpu_tensor.copy_(decoder_output, non_blocking=True)\n            else:\n                prediction_cpu_tensor = decoder_output\n\n            if decoder_lengths is not None and isinstance(decoder_lengths, torch.Tensor):\n                # Before this change, self._greedy_decode_labels would copy\n                # each scalar from GPU to CPU one at a time, in the line:\n                # prediction = prediction[:out_len]\n                # Doing one GPU to CPU copy ahead of time amortizes that overhead.\n                decoder_lengths = decoder_lengths.cpu()\n\n            if prediction_cpu_tensor.ndim < 2 or prediction_cpu_tensor.ndim > 3:\n                raise ValueError(\n                    f\"`decoder_output` must be a tensor of shape [B, T] (labels, int) or \"\n                    f\"[B, T, V] (log probs, float). Provided shape = {prediction_cpu_tensor.shape}\"\n                )\n\n            # determine type of input - logprobs or labels\n            if prediction_cpu_tensor.ndim == 2:  # labels\n                greedy_decode = self._greedy_decode_labels\n            else:\n                greedy_decode = self._greedy_decode_logprobs\n\n            for ind in range(prediction_cpu_tensor.shape[0]):\n                out_len = decoder_lengths[ind] if decoder_lengths is not None else None\n                hypothesis = greedy_decode(prediction_cpu_tensor[ind], out_len)\n                hypotheses.append(hypothesis)\n\n            # Pack results into Hypotheses\n            packed_result = pack_hypotheses(hypotheses, decoder_lengths)\n\n        return (packed_result,)\n\n    @torch.no_grad()\n    def _greedy_decode_logprobs(self, x: torch.Tensor, out_len: Optional[torch.Tensor]):\n        # x: [T, D]\n        # out_len: [seq_len]\n\n        # Initialize blank state and empty label set in Hypothesis\n        hypothesis = rnnt_utils.Hypothesis(score=0.0, y_sequence=[], dec_state=None, timestep=[], last_token=None)\n        prediction = x.cpu()\n\n        if out_len is not None:\n            prediction = prediction[:out_len]\n\n        prediction_logprobs, prediction_labels = prediction.max(dim=-1)\n\n        non_blank_ids = prediction_labels != self.blank_id\n        hypothesis.y_sequence = prediction_labels.tolist()\n        hypothesis.score = (prediction_logprobs[non_blank_ids]).sum()\n\n        if self.preserve_alignments:\n            # Preserve the logprobs, as well as labels after argmax\n            hypothesis.alignments = (prediction.clone(), prediction_labels.clone())\n\n        if self.compute_timestamps:\n            hypothesis.timestep = torch.nonzero(non_blank_ids, as_tuple=False)[:, 0].tolist()\n\n        if self.preserve_frame_confidence:\n            hypothesis.frame_confidence = self._get_confidence(prediction)\n\n        return hypothesis\n\n    @torch.no_grad()\n    def _greedy_decode_labels(self, x: torch.Tensor, out_len: Optional[torch.Tensor]):\n        # x: [T]\n        # out_len: [seq_len]\n\n        # Initialize blank state and empty label set in Hypothesis\n        hypothesis = rnnt_utils.Hypothesis(score=0.0, y_sequence=[], dec_state=None, timestep=[], last_token=None)\n        prediction_labels = x.cpu()\n\n        if out_len is not None:\n            prediction_labels = prediction_labels[:out_len]\n\n        non_blank_ids = prediction_labels != self.blank_id\n        hypothesis.y_sequence = prediction_labels.tolist()\n        hypothesis.score = -1.0\n\n        if self.preserve_alignments:\n            raise ValueError(\"Requested for alignments, but predictions provided were labels, not log probabilities.\")\n\n        if self.compute_timestamps:\n            hypothesis.timestep = torch.nonzero(non_blank_ids, as_tuple=False)[:, 0].tolist()\n\n        if self.preserve_frame_confidence:\n            raise ValueError(\n                \"Requested for per-frame confidence, but predictions provided were labels, not log probabilities.\"\n            )\n\n        return hypothesis\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\nclass GreedyBatchedCTCInfer(Typing, ConfidenceMethodMixin):\n    \"\"\"A vectorized greedy CTC decoder.\n\n    This is basically always faster than GreedyCTCInfer, and supports\n    the same interface. See issue #8891 on github for what is wrong\n    with GreedyCTCInfer. GreedyCTCInfer loops over each element in the\n    batch, running kernels at batch size one. CPU overheads end up\n    dominating. This implementation does appropriate masking to\n    appropriately do the same operation in a batched manner.\n\n    Args:\n        blank_index: int index of the blank token. Can be 0 or len(vocabulary).\n        preserve_alignments: Bool flag which preserves the history of logprobs generated during\n            decoding (sample / batched). When set to true, the Hypothesis will contain\n            the non-null value for `logprobs` in it. Here, `logprobs` is a torch.Tensors.\n        compute_timestamps: A bool flag, which determines whether to compute the character/subword, or\n                word based timestamp mapping the output log-probabilities to discrite intervals of timestamps.\n                The timestamps will be available in the returned Hypothesis.timestep as a dictionary.\n        preserve_frame_confidence: Bool flag which preserves the history of per-frame confidence scores\n            generated during decoding. When set to true, the Hypothesis will contain\n            the non-null value for `frame_confidence` in it. Here, `frame_confidence` is a List of floats.\n        confidence_method_cfg: A dict-like object which contains the method name and settings to compute per-frame\n            confidence scores.\n\n            name: The method name (str).\n                Supported values:\n                    - 'max_prob' for using the maximum token probability as a confidence.\n                    - 'entropy' for using a normalized entropy of a log-likelihood vector.\n\n            entropy_type: Which type of entropy to use (str). Used if confidence_method_cfg.name is set to `entropy`.\n                Supported values:\n                    - 'gibbs' for the (standard) Gibbs entropy. If the alpha () is provided,\n                        the formula is the following: H_ = -sum_i((p^_i)*log(p^_i)).\n                        Note that for this entropy, the alpha should comply the following inequality:\n                        (log(V)+2-sqrt(log^2(V)+4))/(2*log(V)) <=  <= (1+log(V-1))/log(V-1)\n                        where V is the model vocabulary size.\n                    - 'tsallis' for the Tsallis entropy with the Boltzmann constant one.\n                        Tsallis entropy formula is the following: H_ = 1/(-1)*(1-sum_i(p^_i)),\n                        where  is a parameter. When  == 1, it works like the Gibbs entropy.\n                        More: https://en.wikipedia.org/wiki/Tsallis_entropy\n                    - 'renyi' for the Rnyi entropy.\n                        Rnyi entropy formula is the following: H_ = 1/(1-)*log_2(sum_i(p^_i)),\n                        where  is a parameter. When  == 1, it works like the Gibbs entropy.\n                        More: https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy\n\n            alpha: Power scale for logsoftmax ( for entropies). Here we restrict it to be > 0.\n                When the alpha equals one, scaling is not applied to 'max_prob',\n                and any entropy type behaves like the Shannon entropy: H = -sum_i(p_i*log(p_i))\n\n            entropy_norm: A mapping of the entropy value to the interval [0,1].\n                Supported values:\n                    - 'lin' for using the linear mapping.\n                    - 'exp' for using exponential mapping with linear shift.\n\n    \"\"\"\n\n    @property\n    def input_types(self):\n        \"\"\"Returns definitions of module input ports.\"\"\"\n        # Input can be of dimension -\n        # ('B', 'T', 'D') [Log probs] or ('B', 'T') [Labels]\n\n        return {\n            \"decoder_output\": NeuralType(None, LogprobsType()),\n            \"decoder_lengths\": NeuralType(tuple('B'), LengthsType()),\n        }\n\n    @property\n    def output_types(self):\n        \"\"\"Returns definitions of module output ports.\"\"\"\n        return {\"predictions\": [NeuralType(elements_type=HypothesisType())]}\n\n    def __init__(\n        self,\n        blank_id: int,\n        preserve_alignments: bool = False,\n        compute_timestamps: bool = False,\n        preserve_frame_confidence: bool = False,\n        confidence_method_cfg: Optional[DictConfig] = None,\n    ):\n        super().__init__()\n\n        self.blank_id = blank_id\n        self.preserve_alignments = preserve_alignments\n        # we need timestamps to extract non-blank per-frame confidence\n        self.compute_timestamps = compute_timestamps | preserve_frame_confidence\n        self.preserve_frame_confidence = preserve_frame_confidence\n\n        # set confidence calculation method\n        self._init_confidence_method(confidence_method_cfg)\n\n    @typecheck()\n    def forward(\n        self,\n        decoder_output: torch.Tensor,\n        decoder_lengths: Optional[torch.Tensor],\n    ):\n        \"\"\"Returns a list of hypotheses given an input batch of the encoder hidden embedding.\n        Output token is generated auto-repressively.\n\n        Args:\n            decoder_output: A tensor of size (batch, timesteps, features) or (batch, timesteps) (each timestep is a label).\n            decoder_lengths: list of int representing the length of each sequence\n                output sequence.\n\n        Returns:\n            packed list containing batch number of sentences (Hypotheses).\n        \"\"\"\n\n        input_decoder_lengths = decoder_lengths\n\n        if decoder_lengths is None:\n            logging.warning(_DECODER_LENGTHS_NONE_WARNING, mode=logging_mode.ONCE)\n            decoder_lengths = torch.tensor(\n                [decoder_output.shape[1]], dtype=torch.long, device=decoder_output.device\n            ).expand(decoder_output.shape[0])\n\n        # GreedyCTCInfer::forward(), by accident, works with\n        # decoder_lengths on either CPU or GPU when decoder_output is\n        # on GPU. For the sake of backwards compatibility, we also\n        # allow decoder_lengths to be on the CPU device. In this case,\n        # we simply copy the decoder_lengths from CPU to GPU. If both\n        # tensors are already on the same device, this is a no-op.\n        decoder_lengths = decoder_lengths.to(decoder_output.device)\n\n        if decoder_output.ndim == 2:\n            hypotheses = self._greedy_decode_labels_batched(decoder_output, decoder_lengths)\n        else:\n            hypotheses = self._greedy_decode_logprobs_batched(decoder_output, decoder_lengths)\n        packed_result = pack_hypotheses(hypotheses, input_decoder_lengths)\n        return (packed_result,)\n\n    @torch.no_grad()\n    def _greedy_decode_logprobs_batched(self, x: torch.Tensor, out_len: torch.Tensor):\n        # x: [B, T, D]\n        # out_len: [B]\n\n        batch_size = x.shape[0]\n        max_time = x.shape[1]\n\n        predictions = x\n        # In CTC greedy decoding, each output maximum likelihood token\n        # is calculated independent of the other tokens.\n        predictions_logprobs, predictions_labels = predictions.max(dim=-1)\n\n        # Since predictions_logprobs is a padded matrix in the time\n        # dimension, we consider invalid timesteps to be \"blank\".\n        time_steps = torch.arange(max_time, device=x.device).unsqueeze(0).expand(batch_size, max_time)\n        non_blank_ids_mask = torch.logical_and(predictions_labels != self.blank_id, time_steps < out_len.unsqueeze(1))\n        # Sum the non-blank labels to compute the score of the\n        # transcription. This follows from Eq. (3) of \"Connectionist\n        # Temporal Classification: Labelling Unsegmented Sequence Data\n        # with Recurrent Neural Networks\".\n        scores = torch.where(non_blank_ids_mask, predictions_logprobs, 0.0).sum(axis=1)\n\n        scores = scores.cpu()\n        predictions_labels = predictions_labels.cpu()\n        out_len = out_len.cpu()\n\n        if self.preserve_alignments or self.preserve_frame_confidence:\n            predictions = predictions.cpu()\n\n        hypotheses = []\n\n        # This mimics the for loop in GreedyCTCInfer::forward.\n        for i in range(batch_size):\n            hypothesis = rnnt_utils.Hypothesis(score=0.0, y_sequence=[], dec_state=None, timestep=[], last_token=None)\n            hypothesis.score = scores[i]\n\n            prediction_labels_no_padding = predictions_labels[i, : out_len[i]].tolist()\n\n            assert predictions_labels.dtype == torch.int64\n            hypothesis.y_sequence = prediction_labels_no_padding\n\n            if self.preserve_alignments:\n                hypothesis.alignments = (\n                    predictions[i, : out_len[i], :].clone(),\n                    predictions_labels[i, : out_len[i]].clone(),\n                )\n            if self.compute_timestamps:\n                # TOOD: Could do this in a vectorized manner... Would\n                # prefer to have nonzero_static, though, for sanity.\n                # Or do a prefix sum on out_len\n                hypothesis.timestep = torch.nonzero(non_blank_ids_mask[i], as_tuple=False)[:, 0].cpu().tolist()\n            if self.preserve_frame_confidence:\n                hypothesis.frame_confidence = self._get_confidence(predictions[i, : out_len[i], :])\n\n            hypotheses.append(hypothesis)\n\n        return hypotheses\n\n    @torch.no_grad()\n    def _greedy_decode_labels_batched(self, x: torch.Tensor, out_len: torch.Tensor):\n        \"\"\"\n        This does greedy decoding in the case where you have already found the\n        most likely token at each timestep.\n        \"\"\"\n        # x: [B, T]\n        # out_len: [B]\n\n        batch_size = x.shape[0]\n        max_time = x.shape[1]\n\n        predictions_labels = x\n        time_steps = torch.arange(max_time, device=x.device).unsqueeze(0).expand(batch_size, max_time)\n        non_blank_ids_mask = torch.logical_and(predictions_labels != self.blank_id, time_steps < out_len.unsqueeze(1))\n        predictions_labels = predictions_labels.cpu()\n        out_len = out_len.cpu()\n\n        hypotheses = []\n\n        for i in range(batch_size):\n            hypothesis = rnnt_utils.Hypothesis(score=0.0, y_sequence=[], dec_state=None, timestep=[], last_token=None)\n            hypothesis.y_sequence = predictions_labels[i, : out_len[i]].tolist()\n            hypothesis.score = -1.0\n\n            if self.preserve_alignments:\n                raise ValueError(\n                    \"Requested for alignments, but predictions provided were labels, not log probabilities.\"\n                )\n            if self.compute_timestamps:\n                # TOOD: Could do this in a vectorized manner... Would\n                # prefer to have nonzero_static, though, for sanity.\n                # Or do a prefix sum on out_len\n                hypothesis.timestep = torch.nonzero(non_blank_ids_mask[i], as_tuple=False)[:, 0].cpu().tolist()\n            if self.preserve_frame_confidence:\n                raise ValueError(\n                    \"Requested for per-frame confidence, but predictions provided were labels, not log probabilities.\"\n                )\n\n            hypotheses.append(hypothesis)\n\n        return hypotheses\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\n@dataclass\nclass GreedyCTCInferConfig:\n    preserve_alignments: bool = False\n    compute_timestamps: bool = False\n    preserve_frame_confidence: bool = False\n    confidence_method_cfg: Optional[ConfidenceMethodConfig] = field(default_factory=lambda: ConfidenceMethodConfig())\n\n    def __post_init__(self):\n        # OmegaConf.structured ensures that post_init check is always executed\n        self.confidence_method_cfg = OmegaConf.structured(\n            self.confidence_method_cfg\n            if isinstance(self.confidence_method_cfg, ConfidenceMethodConfig)\n            else ConfidenceMethodConfig(**self.confidence_method_cfg)\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py nemo_model_file=\"/kaggle/working/final_asr_model.nemo\" \\\n       input_manifest=\"/kaggle/input/dataset-ja/banana.json\" \\\n       kenlm_model_file=\"/kaggle/working/kenlm_model.binary\" \\\n       beam_width=[64,128] \\\n       beam_alpha=[1.0] \\\n       beam_beta=[1.0,0.5]\\\n       preds_output_folder=\"/kaggle/working/predictions\" \\\n       probs_cache_file=null \\\n       decoding_mode=beamsearch_ngram \\\n       decoding_strategy=\"beam\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py \\\n       nemo_model_file=\"/kaggle/working/final_asr_model.nemo\" \\\n       input_manifest=\"/kaggle/working/output_file.json\" \\\n       kenlm_model_file=\"/kaggle/working/kenlm_model.binary\" \\\n       beam_width=[64,128] \\\n       beam_alpha=[1.0] \\\n       beam_beta=[1.0,0.5]\\\n       preds_output_folder=\"/kaggle/working/predictions\" \\\n       probs_cache_file=null \\\n       decoding_mode=beamsearch_ngram \\\n       decoding_strategy=\"beam\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cd /kaggle/working/NeMo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Flashlight\n#!git clone https://github.com/flashlight/text\n#cd text\n#!python setup.py bdist_wheel  # Build the wheel file\n#!pip install dist/*.whl       # Install the built wheel\n#cd ..\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cd /kaggle/working/NeMo/text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!python setup.py bdist_wheel  # Build the wheel file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install dist/*.whl       # Install the built wheel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get remove -y cmake\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/Kitware/CMake/releases/download/v3.24.0/cmake-3.24.0-linux-x86_64.tar.gz\n!tar -xzvf cmake-3.24.0-linux-x86_64.tar.gz\n!sudo cp -r cmake-3.24.0-linux-x86_64/* /usr/local/\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cmake --version\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Flashlight\ngit clone https://github.com/flashlight/text\ncd text\npython setup.py bdist_wheel  # Build the wheel file\npip install dist/*.whl       # Install the built wheel\ncd ..\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['HYDRA_FULL_ERROR'] = '1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/train_kenlm.py\n# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# This script would train an N-gram language model with KenLM library (https://github.com/kpu/kenlm) which can be used\n# with the beam search decoders on top of the ASR models. This script supports both character level and BPE level\n# encodings and models which is detected automatically from the type of the model.\n# After the N-gram model is trained, and stored in the binary format, you may use\n# 'scripts/ngram_lm/eval_beamsearch_ngram.py' to evaluate it on an ASR model.\n#\n# You need to install the KenLM library and also the beam search decoders to use this feature. Please refer\n# to 'scripts/ngram_lm/install_beamsearch_decoders.sh' on how to install them.\n#\n# USAGE: python train_kenlm.py nemo_model_file=<path to the .nemo file of the model> \\\n#                              train_paths=<list of paths to the training text or JSON manifest file> \\\n#                              kenlm_bin_path=<path to the bin folder of KenLM library> \\\n#                              kenlm_model_file=<path to store the binary KenLM model> \\\n#                              ngram_length=<order of N-gram model> \\\n#\n# After training is done, the binary LM model is stored at the path specified by '--kenlm_model_file'.\n# You may find more info on how to use this script at:\n# https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html\n\nimport logging\nimport os\nos.environ['HYDRA_FULL_ERROR'] = '1'\nimport subprocess\nimport sys\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom typing import List\n\nfrom omegaconf import MISSING\n\n# Update the Python path to include the scripts directory\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..')))\n\nfrom scripts.asr_language_modeling.ngram_lm import kenlm_utils\n\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\n\n\"\"\"\nNeMo's beam search decoders only support char-level encodings. In order to make it work with BPE-level encodings, we\nuse a trick to encode the sub-word tokens of the training data as unicode characters and train a char-level KenLM. \n\"\"\"\n\n\n@dataclass\nclass TrainKenlmConfig:\n    \"\"\"\n    Train an N-gram language model with KenLM to be used with beam search decoder of ASR models.\n    \"\"\"\n\n    train_paths: List[\n        str\n    ] = MISSING  # List of training files or folders. Files can be a plain text file or \".json\" manifest or \".json.gz\". Example: [/path/to/manifest/file,/path/to/folder]\n\n    nemo_model_file: str = MISSING  # The path to '.nemo' file of the ASR model, or name of a pretrained NeMo model\n    kenlm_model_file: str = MISSING  # The path to store the KenLM binary model file\n    ngram_length: int = MISSING  # The order of N-gram LM\n    kenlm_bin_path: str = MISSING  # The path to the bin folder of KenLM.\n\n    preserve_arpa: bool = False  # Whether to preserve the intermediate ARPA file.\n    ngram_prune: List[int] = field(\n        default_factory=lambda: [0]\n    )  # List of digits to prune Ngram. Example: [0,0,1]. See Pruning section on the https://kheafield.com/code/kenlm/estimation\n    cache_path: str = \"\"  # Cache path to save tokenized files.\n    verbose: int = 1  # Verbose level, default is 1.\n\n\n@hydra_runner(config_path=None, config_name='TrainKenlmConfig', schema=TrainKenlmConfig)\ndef main(args: TrainKenlmConfig):\n    train_paths = kenlm_utils.get_train_list(args.train_paths)\n\n    if isinstance(args.ngram_prune, str):\n        args.ngram_prune = [args.ngram_prune]\n\n    tokenizer, encoding_level, is_aggregate_tokenizer = kenlm_utils.setup_tokenizer(args.nemo_model_file)\n\n    if encoding_level == \"subword\":\n        discount_arg = \"--discount_fallback\"  # --discount_fallback is needed for training KenLM for BPE-based models\n    else:\n        discount_arg = \"\"\n\n    arpa_file = f\"{args.kenlm_model_file}.tmp.arpa\"\n    \"\"\" LMPLZ ARGUMENT SETUP \"\"\"\n    kenlm_args = [\n        os.path.join(args.kenlm_bin_path, 'lmplz'),\n        \"-o\",\n        str(args.ngram_length),\n        \"--arpa\",\n        arpa_file,\n        discount_arg,\n        \"--prune\",\n    ] + [str(n) for n in args.ngram_prune]\n\n    if args.cache_path:\n        if not os.path.exists(args.cache_path):\n            os.makedirs(args.cache_path, exist_ok=True)\n\n        \"\"\" DATASET SETUP \"\"\"\n        encoded_train_files = []\n        for file_num, train_file in enumerate(train_paths):\n            logging.info(f\"Encoding the train file '{train_file}' number {file_num+1} out of {len(train_paths)} ...\")\n\n            cached_files = glob(os.path.join(args.cache_path, os.path.split(train_file)[1]) + \"*\")\n            encoded_train_file = os.path.join(args.cache_path, os.path.split(train_file)[1] + f\"_{file_num}.tmp.txt\")\n            if (\n                cached_files and cached_files[0] != encoded_train_file\n            ):  # cached_files exists but has another file name: f\"_{file_num}.tmp.txt\"\n                os.rename(cached_files[0], encoded_train_file)\n                logging.info(\"Rename\", cached_files[0], \"to\", encoded_train_file)\n\n            encoded_train_files.append(encoded_train_file)\n\n        kenlm_utils.iter_files(\n            source_path=train_paths,\n            dest_path=encoded_train_files,\n            tokenizer=tokenizer,\n            encoding_level=encoding_level,\n            is_aggregate_tokenizer=is_aggregate_tokenizer,\n            verbose=args.verbose,\n        )\n\n        first_process_args = [\"cat\"] + encoded_train_files\n        first_process = subprocess.Popen(first_process_args, stdout=subprocess.PIPE, stderr=sys.stderr)\n\n        logging.info(f\"Running lmplz command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n        kenlm_p = subprocess.run(\n            kenlm_args,\n            stdin=first_process.stdout,\n            capture_output=False,\n            text=True,\n            stdout=sys.stdout,\n            stderr=sys.stderr,\n        )\n        first_process.wait()\n\n    else:\n        logging.info(f\"Running lmplz command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n        kenlm_p = subprocess.Popen(kenlm_args, stdout=sys.stdout, stdin=subprocess.PIPE, stderr=sys.stderr)\n\n        kenlm_utils.iter_files(\n            source_path=train_paths,\n            dest_path=kenlm_p.stdin,\n            tokenizer=tokenizer,\n            encoding_level=encoding_level,\n            is_aggregate_tokenizer=is_aggregate_tokenizer,\n            verbose=args.verbose,\n        )\n\n        kenlm_p.communicate()\n\n    if kenlm_p.returncode != 0:\n        raise RuntimeError(\"Training KenLM was not successful!\")\n\n    \"\"\" BINARY BUILD \"\"\"\n\n    kenlm_args = [\n        os.path.join(args.kenlm_bin_path, \"build_binary\"),\n        \"trie\",\n        arpa_file,\n        args.kenlm_model_file,\n    ]\n    logging.info(f\"Running binary_build command \\n\\n{' '.join(kenlm_args)}\\n\\n\")\n    ret = subprocess.run(kenlm_args, capture_output=False, text=True, stdout=sys.stdout, stderr=sys.stderr)\n\n    if ret.returncode != 0:\n        raise RuntimeError(\"Training KenLM was not successful!\")\n\n    if not args.preserve_arpa:\n        os.remove(arpa_file)\n        logging.info(f\"Deleted the arpa file '{arpa_file}'.\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['HYDRA_FULL_ERROR'] = '1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py\nimport contextlib\nimport json\nimport os\n\nos.environ['HYDRA_FULL_ERROR'] = '1'\nimport pickle\nimport tempfile\nfrom dataclasses import dataclass, field, is_dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport editdistance\nimport numpy as np\nimport torch\nfrom omegaconf import MISSING, OmegaConf\nfrom sklearn.model_selection import ParameterGrid\nfrom tqdm.auto import tqdm\n\nimport nemo.collections.asr as nemo_asr\nfrom nemo.collections.asr.parts.submodules import rnnt_beam_decoding\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\n\n# fmt: off\n\n\n@dataclass\nclass EvalBeamSearchNGramConfig:\n    nemo_model_file: str = MISSING\n    input_manifest: str = MISSING\n    decoding_mode: str = MISSING  # Add this line\n    kenlm_model_file: Optional[str] = None\n    preds_output_folder: Optional[str] = None\n    probs_cache_file: Optional[str] = None\n\n    acoustic_batch_size: int = 128\n    beam_batch_size: int = 128\n    device: str = \"cuda\"\n    use_amp: bool = False\n    num_workers: int = 1\n\n    decoding_strategy: str = \"beam\"\n    beam_width: List[int] = field(default_factory=lambda: [128, 256])\n    beam_alpha: List[float] = field(default_factory=lambda: [0.5, 1.0])\n    maes_prefix_alpha: List[int] = field(default_factory=lambda: [2])\n    maes_expansion_gamma: List[float] = field(default_factory=lambda: [2.3])\n    hat_subtract_ilm: bool = False\n    hat_ilm_weight: List[float] = field(default_factory=lambda: [0.0])\n\n    decoding: rnnt_beam_decoding.BeamRNNTInferConfig = field(default_factory=lambda: rnnt_beam_decoding.BeamRNNTInferConfig(beam_size=128))\n\n\n# fmt: on\n\n\ndef decoding_step(\n    model: nemo_asr.models.ASRModel,\n    cfg: EvalBeamSearchNGramConfig,\n    all_probs: List[torch.Tensor],\n    target_transcripts: List[str],\n    preds_output_file: str = None,\n    beam_batch_size: int = 128,\n    progress_bar: bool = True,\n):\n    level = logging.getEffectiveLevel()\n    logging.setLevel(logging.CRITICAL)\n    model.change_decoding_strategy(None)\n\n    cfg.decoding.hat_ilm_weight = cfg.decoding.hat_ilm_weight * cfg.hat_subtract_ilm\n    cfg.decoding.return_best_hypothesis = False\n    cfg.decoding.ngram_lm_model = cfg.kenlm_model_file\n    cfg.decoding.hat_subtract_ilm = cfg.hat_subtract_ilm\n\n    model.cfg.decoding.strategy = cfg.decoding_strategy\n    model.cfg.decoding.beam = cfg.decoding\n    model.change_decoding_strategy(model.cfg.decoding)\n    logging.setLevel(level)\n\n    wer_dist_first = cer_dist_first = 0\n    wer_dist_best = cer_dist_best = 0\n    words_count = 0\n    chars_count = 0\n    sample_idx = 0\n    if preds_output_file:\n        out_file = open(preds_output_file, 'w', encoding='utf_8', newline='\\n')\n\n    if progress_bar:\n        if cfg.decoding_strategy == \"greedy_batch\":\n            description = \"Greedy_batch decoding..\"\n        else:\n            description = f\"{cfg.decoding_strategy} decoding with bw={cfg.decoding.beam_size}, ba={cfg.decoding.ngram_lm_alpha}, ma={cfg.decoding.maes_prefix_alpha}, mg={cfg.decoding.maes_expansion_gamma}, hat_ilmw={cfg.decoding.hat_ilm_weight}\"\n        it = tqdm(range(int(np.ceil(len(all_probs) / beam_batch_size))), desc=description, ncols=120)\n    else:\n        it = range(int(np.ceil(len(all_probs) / beam_batch_size)))\n    for batch_idx in it:\n        probs_batch = all_probs[batch_idx * beam_batch_size : (batch_idx + 1) * beam_batch_size]\n        probs_lens = torch.tensor([prob.shape[-1] for prob in probs_batch])\n        with torch.no_grad():\n            packed_batch = torch.zeros(len(probs_batch), probs_batch[0].shape[0], max(probs_lens), device='cpu')\n            for prob_index in range(len(probs_batch)):\n                packed_batch[prob_index, :, : probs_lens[prob_index]] = torch.tensor(\n                    probs_batch[prob_index].unsqueeze(0), device=packed_batch.device, dtype=packed_batch.dtype\n                )\n            best_hyp_batch, beams_batch = model.decoding.rnnt_decoder_predictions_tensor(\n                packed_batch, probs_lens, return_hypotheses=True,\n            )\n        if cfg.decoding_strategy == \"greedy_batch\":\n            beams_batch = [[x] for x in best_hyp_batch]\n\n        for beams_idx, beams in enumerate(beams_batch):\n            target = target_transcripts[sample_idx + beams_idx]\n            target_split_w = target.split()\n            target_split_c = list(target)\n            words_count += len(target_split_w)\n            chars_count += len(target_split_c)\n            wer_dist_min = cer_dist_min = 10000\n            for candidate_idx, candidate in enumerate(beams):\n                pred_text = candidate.text\n                pred_split_w = pred_text.split()\n                wer_dist = editdistance.eval(target_split_w, pred_split_w)\n                pred_split_c = list(pred_text)\n                cer_dist = editdistance.eval(target_split_c, pred_split_c)\n\n                wer_dist_min = min(wer_dist_min, wer_dist)\n                cer_dist_min = min(cer_dist_min, cer_dist)\n\n                if candidate_idx == 0:\n                    wer_dist_first += wer_dist\n                    cer_dist_first += cer_dist\n\n                score = candidate.score\n                if preds_output_file:\n                    out_file.write('{}\\t{}\\n'.format(pred_text, score))\n            wer_dist_best += wer_dist_min\n            cer_dist_best += cer_dist_min\n        sample_idx += len(probs_batch)\n\n    if cfg.decoding_strategy == \"greedy_batch\":\n        return wer_dist_first / words_count, cer_dist_first / chars_count\n\n    if preds_output_file:\n        out_file.close()\n        logging.info(f\"Stored the predictions of {cfg.decoding_strategy} decoding at '{preds_output_file}'.\")\n\n    if cfg.decoding.ngram_lm_model:\n        logging.info(\n            f\"WER/CER with {cfg.decoding_strategy} decoding and N-gram model = {wer_dist_first / words_count:.2%}/{cer_dist_first / chars_count:.2%}\"\n        )\n    else:\n        logging.info(\n            f\"WER/CER with {cfg.decoding_strategy} decoding = {wer_dist_first / words_count:.2%}/{cer_dist_first / chars_count:.2%}\"\n        )\n    logging.info(\n        f\"Oracle WER/CER in candidates with perfect LM= {wer_dist_best / words_count:.2%}/{cer_dist_best / chars_count:.2%}\"\n    )\n    logging.info(f\"=================================================================================\")\n\n    return wer_dist_first / words_count, cer_dist_first / chars_count\n\n\n@hydra_runner(config_path=None, config_name='EvalBeamSearchNGramConfig', schema=EvalBeamSearchNGramConfig)\ndef main(cfg: EvalBeamSearchNGramConfig):\n    if is_dataclass(cfg):\n        cfg = OmegaConf.structured(cfg)  # type: EvalBeamSearchNGramConfig\n\n    valid_decoding_strategis = [\"greedy_batch\", \"beam\", \"tsd\", \"alsd\", \"maes\"]\n    if cfg.decoding_strategy not in valid_decoding_strategis:\n        raise ValueError(\n            f\"Given decoding_strategy={cfg.decoding_strategy} is invalid. Available options are :\\n\"\n            f\"{valid_decoding_strategis}\"\n        )\n\n    if cfg.nemo_model_file.endswith('.nemo'):\n        asr_model = nemo_asr.models.ASRModel.restore_from(cfg.nemo_model_file, map_location=torch.device(cfg.device))\n    else:\n        logging.warning(\n            \"nemo_model_file does not end with .nemo, therefore trying to load a pretrained model with this name.\"\n        )\n        asr_model = nemo_asr.models.ASRModel.from_pretrained(\n            cfg.nemo_model_file, map_location=torch.device(cfg.device)\n        )\n\n    if cfg.kenlm_model_file:\n        if not os.path.exists(cfg.kenlm_model_file):\n            raise FileNotFoundError(f\"Could not find the KenLM model file '{cfg.kenlm_model_file}'.\")\n        if cfg.decoding_strategy != \"maes\":\n            raise ValueError(f\"Decoding with kenlm model is supported only for maes decoding algorithm.\")\n        lm_path = cfg.kenlm_model_file\n    else:\n        lm_path = None\n        cfg.beam_alpha = [0.0]\n    if cfg.hat_subtract_ilm:\n        assert lm_path, \"kenlm must be set for hat internal lm subtraction\"\n\n    if cfg.decoding_strategy != \"maes\":\n        cfg.maes_expansion_gamma = [1.0]\n        cfg.maes_prefix_alpha = [1.0]\n\n    if cfg.device == \"cuda\" and not torch.cuda.is_available():\n        logging.warning(\"You have set device=cuda but no CUDA devices found. Setting device=cpu instead.\")\n        cfg.device = \"cpu\"\n    elif cfg.device == \"cpu\" and torch.cuda.is_available():\n        logging.warning(\"You have set device=cpu, but there are available CUDA devices. Using CPU for inference.\")\n\n    if cfg.probs_cache_file and os.path.exists(cfg.probs_cache_file):\n        logging.info(f\"Restoring the probs cache from '{cfg.probs_cache_file}'.\")\n        with open(cfg.probs_cache_file, \"rb\") as cache_f:\n            probs_dict = pickle.load(cache_f)\n    else:\n        logging.info(f\"Computing and caching the probabilities of samples in '{cfg.input_manifest}'.\")\n        probs_dict = {}\n        for test_batch in asr_model.transcribe(\n            paths2audio_files=cfg.input_manifest,\n            batch_size=cfg.acoustic_batch_size,\n            num_workers=cfg.num_workers,\n            return_hypotheses=False,\n            use_amp=cfg.use_amp,\n            channel_selector=None,\n        ):\n            for test_pred in test_batch:\n                probs_dict[test_pred.audio_file] = (test_pred.feature_probs.cpu(), test_pred.tokens.cpu())\n        if cfg.probs_cache_file:\n            with open(cfg.probs_cache_file, \"wb\") as cache_f:\n                pickle.dump(probs_dict, cache_f)\n\n    for grid_idx, params in enumerate(ParameterGrid(cfg.dict_config)):\n        for k, v in params.items():\n            OmegaConf.update(cfg, k, v, merge=True)\n\n        preds_output_file = None\n        if cfg.preds_output_folder:\n            preds_output_file = os.path.join(\n                cfg.preds_output_folder,\n                f\"beam_search_preds_bs={cfg.decoding.beam_size}_ba={cfg.decoding.ngram_lm_alpha}_ma={cfg.decoding.maes_prefix_alpha}_mg={cfg.decoding.maes_expansion_gamma}_ilmw={cfg.decoding.hat_ilm_weight}.txt\",\n            )\n            os.makedirs(cfg.preds_output_folder, exist_ok=True)\n\n        all_probs = []\n        target_transcripts = []\n        for file, (probs, _) in probs_dict.items():\n            all_probs.append(probs)\n            target_transcripts.append(file)\n\n        wer, cer = decoding_step(\n            asr_model,\n            cfg,\n            all_probs,\n            target_transcripts,\n            preds_output_file=preds_output_file,\n            beam_batch_size=cfg.beam_batch_size,\n        )\n        logging.info(f\"WER/CER = {wer:.2%}/{cer:.2%}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\"\"\"\n# This script would evaluate an N-gram language model trained with KenLM library (https://github.com/kpu/kenlm) in\n# fusion with beam search decoders on top of a trained ASR model with CTC decoder. To evaluate a model with \n# Transducer (RNN-T) decoder use another script 'scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_transducer.py'. \n# NeMo's beam search decoders are capable of using the KenLM's N-gram models\n# to find the best candidates. This script supports both character level and BPE level\n# encodings and models which is detected automatically from the type of the model.\n# You may train the LM model with 'scripts/asr_language_modeling/ngram_lm/train_kenlm.py'.\n\n# Config Help\n\nTo discover all arguments of the script, please run :\npython eval_beamsearch_ngram_ctc.py --help\npython eval_beamsearch_ngram_ctc.py --cfg job\n\n# USAGE\n\npython eval_beamsearch_ngram_ctc.py nemo_model_file=<path to the .nemo file of the model> \\\n           input_manifest=<path to the evaluation JSON manifest file> \\\n           kenlm_model_file=<path to the binary KenLM model> \\\n           beam_width=[<list of the beam widths, separated with commas>] \\\n           beam_alpha=[<list of the beam alphas, separated with commas>] \\\n           beam_beta=[<list of the beam betas, separated with commas>] \\\n           preds_output_folder=<optional folder to store the predictions> \\\n           probs_cache_file=null \\\n           decoding_mode=beamsearch_ngram\n           ...\n\n\n# Grid Search for Hyper parameters\n\nFor grid search, you can provide a list of arguments as follows -\n\n           beam_width=[4,8,16,....] \\\n           beam_alpha=[-2.0,-1.0,...,1.0,2.0] \\\n           beam_beta=[-1.0,-0.5,0.0,...,1.0] \\\n\n# You may find more info on how to use this script at:\n# https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html\n\n\"\"\"\n\n\nimport contextlib\nimport json\nimport os\nimport pickle\nfrom dataclasses import dataclass, field, is_dataclass\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport editdistance\nimport numpy as np\nimport torch\nfrom omegaconf import MISSING, OmegaConf\nfrom sklearn.model_selection import ParameterGrid\nfrom tqdm.auto import tqdm\n\nimport nemo.collections.asr as nemo_asr\nfrom nemo.collections.asr.models import EncDecHybridRNNTCTCModel\nfrom nemo.collections.asr.parts.submodules import ctc_beam_decoding\nfrom nemo.collections.asr.parts.utils.transcribe_utils import PunctuationCapitalization, TextProcessingConfig\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\n\n# fmt: off\n\n\n@dataclass\nclass EvalBeamSearchNGramConfig:\n    \"\"\"\n    Evaluate an ASR model with beam search decoding and n-gram KenLM language model.\n    \"\"\"\n    # # The path of the '.nemo' file of the ASR model or the name of a pretrained model (ngc / huggingface)\n    nemo_model_file: str = MISSING\n\n    # File paths\n    input_manifest: str = MISSING  # The manifest file of the evaluation set\n    kenlm_model_file: Optional[str] = None  # The path of the KenLM binary model file\n    preds_output_folder: Optional[str] = None  # The optional folder where the predictions are stored\n    probs_cache_file: Optional[str] = None  # The cache file for storing the logprobs of the model\n\n    # Parameters for inference\n    acoustic_batch_size: int = 16  # The batch size to calculate log probabilities\n    beam_batch_size: int = 128  # The batch size to be used for beam search decoding\n    device: str = \"cuda\"  # The device to load the model onto to calculate log probabilities\n    use_amp: bool = False  # Whether to use AMP if available to calculate log probabilities\n\n    # Beam Search hyperparameters\n\n    # The decoding scheme to be used for evaluation.\n    # Can be one of [\"greedy\", \"beamsearch\", \"beamsearch_ngram\"]\n    decoding_mode: str = \"beamsearch_ngram\"\n\n    beam_width: List[int] = field(default_factory=lambda: [128])  # The width or list of the widths for the beam search decoding\n    beam_alpha: List[float] = field(default_factory=lambda: [1.0])  # The alpha parameter or list of the alphas for the beam search decoding\n    beam_beta: List[float] = field(default_factory=lambda: [0.0])  # The beta parameter or list of the betas for the beam search decoding\n\n    decoding_strategy: str = \"beam\"\n    decoding: ctc_beam_decoding.BeamCTCInferConfig = field(default_factory=lambda: ctc_beam_decoding.BeamCTCInferConfig(beam_size=128))\n    \n    text_processing: Optional[TextProcessingConfig] = field(default_factory=lambda: TextProcessingConfig(\n        punctuation_marks = \".,?\",\n        separate_punctuation = False,\n        do_lowercase = False,\n        rm_punctuation = False,\n    ))\n# fmt: on\n\n\ndef beam_search_eval(\n    model: nemo_asr.models.ASRModel,\n    cfg: EvalBeamSearchNGramConfig,\n    all_probs: List[torch.Tensor],\n    target_transcripts: List[str],\n    preds_output_file: str = None,\n    lm_path: str = None,\n    beam_alpha: float = 1.0,\n    beam_beta: float = 0.0,\n    beam_width: int = 128,\n    beam_batch_size: int = 128,\n    progress_bar: bool = True,\n    punctuation_capitalization: PunctuationCapitalization = None,\n):\n    level = logging.getEffectiveLevel()\n    logging.setLevel(logging.CRITICAL)\n    # Reset config\n    if isinstance(model, EncDecHybridRNNTCTCModel):\n        model.change_decoding_strategy(decoding_cfg=None, decoder_type=\"ctc\")\n    else:\n        model.change_decoding_strategy(None)\n\n    # Override the beam search config with current search candidate configuration\n    cfg.decoding.beam_size = beam_width\n    cfg.decoding.beam_alpha = beam_alpha\n    cfg.decoding.beam_beta = beam_beta\n    cfg.decoding.return_best_hypothesis = False\n    cfg.decoding.kenlm_path = cfg.kenlm_model_file\n\n    # Update model's decoding strategy config\n    model.cfg.decoding.strategy = cfg.decoding_strategy\n    model.cfg.decoding.beam = cfg.decoding\n\n    # Update model's decoding strategy\n    if isinstance(model, EncDecHybridRNNTCTCModel):\n        model.change_decoding_strategy(model.cfg.decoding, decoder_type='ctc')\n        decoding = model.ctc_decoding\n    else:\n        model.change_decoding_strategy(model.cfg.decoding)\n        decoding = model.decoding\n    logging.setLevel(level)\n\n    wer_dist_first = cer_dist_first = 0\n    wer_dist_best = cer_dist_best = 0\n    words_count = 0\n    chars_count = 0\n    sample_idx = 0\n    if preds_output_file:\n        out_file = open(preds_output_file, 'w', encoding='utf_8', newline='\\n')\n\n    if progress_bar:\n        it = tqdm(\n            range(int(np.ceil(len(all_probs) / beam_batch_size))),\n            desc=f\"Beam search decoding with width={beam_width}, alpha={beam_alpha}, beta={beam_beta}\",\n            ncols=120,\n        )\n    else:\n        it = range(int(np.ceil(len(all_probs) / beam_batch_size)))\n    for batch_idx in it:\n        # disabling type checking\n        probs_batch = all_probs[batch_idx * beam_batch_size : (batch_idx + 1) * beam_batch_size]\n        probs_lens = torch.tensor([prob.shape[0] for prob in probs_batch])\n        with torch.no_grad():\n            packed_batch = torch.zeros(len(probs_batch), max(probs_lens), probs_batch[0].shape[-1], device='cpu')\n\n            for prob_index in range(len(probs_batch)):\n                packed_batch[prob_index, : probs_lens[prob_index], :] = torch.tensor(\n                    probs_batch[prob_index], device=packed_batch.device, dtype=packed_batch.dtype\n                )\n\n            _, beams_batch = decoding.ctc_decoder_predictions_tensor(\n                packed_batch, decoder_lengths=probs_lens, return_hypotheses=True,\n            )\n\n        for beams_idx, beams in enumerate(beams_batch):\n            target = target_transcripts[sample_idx + beams_idx]\n            target_split_w = target.split()\n            target_split_c = list(target)\n            words_count += len(target_split_w)\n            chars_count += len(target_split_c)\n            wer_dist_min = cer_dist_min = 10000\n            for candidate_idx, candidate in enumerate(beams):  # type: (int, ctc_beam_decoding.rnnt_utils.Hypothesis)\n                pred_text = candidate.text\n                if cfg.text_processing.do_lowercase:\n                    pred_text = punctuation_capitalization.do_lowercase([pred_text])[0]\n                if cfg.text_processing.rm_punctuation:\n                    pred_text = punctuation_capitalization.rm_punctuation([pred_text])[0]\n                if cfg.text_processing.separate_punctuation:\n                    pred_text = punctuation_capitalization.separate_punctuation([pred_text])[0]\n                pred_split_w = pred_text.split()\n                wer_dist = editdistance.eval(target_split_w, pred_split_w)\n                pred_split_c = list(pred_text)\n                cer_dist = editdistance.eval(target_split_c, pred_split_c)\n\n                wer_dist_min = min(wer_dist_min, wer_dist)\n                cer_dist_min = min(cer_dist_min, cer_dist)\n\n                if candidate_idx == 0:\n                    # first candidate\n                    wer_dist_first += wer_dist\n                    cer_dist_first += cer_dist\n\n                score = candidate.score\n                if preds_output_file:\n                    out_file.write('{}\\t{}\\n'.format(pred_text, score))\n            wer_dist_best += wer_dist_min\n            cer_dist_best += cer_dist_min\n        sample_idx += len(probs_batch)\n\n    if preds_output_file:\n        out_file.close()\n        logging.info(f\"Stored the predictions of beam search decoding at '{preds_output_file}'.\")\n\n    if lm_path:\n        logging.info(\n            'WER/CER with beam search decoding and N-gram model = {:.2%}/{:.2%}'.format(\n                wer_dist_first / words_count, cer_dist_first / chars_count\n            )\n        )\n    else:\n        logging.info(\n            'WER/CER with beam search decoding = {:.2%}/{:.2%}'.format(\n                wer_dist_first / words_count, cer_dist_first / chars_count\n            )\n        )\n    logging.info(\n        'Oracle WER/CER in candidates with perfect LM= {:.2%}/{:.2%}'.format(\n            wer_dist_best / words_count, cer_dist_best / chars_count\n        )\n    )\n    logging.info(f\"=================================================================================\")\n\n    return wer_dist_first / words_count, cer_dist_first / chars_count\n\n\n@hydra_runner(config_path=None, config_name='EvalBeamSearchNGramConfig', schema=EvalBeamSearchNGramConfig)\ndef main(cfg: EvalBeamSearchNGramConfig):\n    if is_dataclass(cfg):\n        cfg = OmegaConf.structured(cfg)  # type: EvalBeamSearchNGramConfig\n\n    valid_decoding_modes = [\"greedy\", \"beamsearch\", \"beamsearch_ngram\"]\n    if cfg.decoding_mode not in valid_decoding_modes:\n        raise ValueError(\n            f\"Given decoding_mode={cfg.decoding_mode} is invalid. Available options are :\\n\" f\"{valid_decoding_modes}\"\n        )\n\n    if cfg.nemo_model_file.endswith('.nemo'):\n        asr_model = nemo_asr.models.ASRModel.restore_from(cfg.nemo_model_file, map_location=torch.device(cfg.device))\n    else:\n        logging.warning(\n            \"nemo_model_file does not end with .nemo, therefore trying to load a pretrained model with this name.\"\n        )\n        asr_model = nemo_asr.models.ASRModel.from_pretrained(\n            cfg.nemo_model_file, map_location=torch.device(cfg.device)\n        )\n\n    target_transcripts = []\n    manifest_dir = Path(cfg.input_manifest).parent\n    with open(cfg.input_manifest, 'r', encoding='utf_8') as manifest_file:\n        audio_file_paths = []\n        for line in tqdm(manifest_file, desc=f\"Reading Manifest {cfg.input_manifest} ...\", ncols=120):\n            data = json.loads(line)\n            audio_file = Path(data['audio_filepath'])\n            if not audio_file.is_file() and not audio_file.is_absolute():\n                audio_file = manifest_dir / audio_file\n            target_transcripts.append(data['text'])\n            audio_file_paths.append(str(audio_file.absolute()))\n\n    punctuation_capitalization = PunctuationCapitalization(cfg.text_processing.punctuation_marks)\n    if cfg.text_processing.do_lowercase:\n        target_transcripts = punctuation_capitalization.do_lowercase(target_transcripts)\n    if cfg.text_processing.rm_punctuation:\n        target_transcripts = punctuation_capitalization.rm_punctuation(target_transcripts)\n    if cfg.text_processing.separate_punctuation:\n        target_transcripts = punctuation_capitalization.separate_punctuation(target_transcripts)\n\n    if cfg.probs_cache_file and os.path.exists(cfg.probs_cache_file):\n        logging.info(f\"Found a pickle file of probabilities at '{cfg.probs_cache_file}'.\")\n        logging.info(f\"Loading the cached pickle file of probabilities from '{cfg.probs_cache_file}' ...\")\n        with open(cfg.probs_cache_file, 'rb') as probs_file:\n            all_probs = pickle.load(probs_file)\n\n        if len(all_probs) != len(audio_file_paths):\n            raise ValueError(\n                f\"The number of samples in the probabilities file '{cfg.probs_cache_file}' does not \"\n                f\"match the manifest file. You may need to delete the probabilities cached file.\"\n            )\n    else:\n\n        @contextlib.contextmanager\n        def default_autocast():\n            yield\n\n        if cfg.use_amp:\n            if torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):\n                logging.info(\"AMP is enabled!\\n\")\n                autocast = torch.cuda.amp.autocast\n\n            else:\n                autocast = default_autocast\n        else:\n\n            autocast = default_autocast\n\n        with autocast():\n            with torch.no_grad():\n                if isinstance(asr_model, EncDecHybridRNNTCTCModel):\n                    asr_model.cur_decoder = 'ctc'\n                all_logits = asr_model.transcribe(audio_file_paths, batch_size=cfg.acoustic_batch_size)\n\n\n        all_probs = all_logits\n        if cfg.probs_cache_file:\n            os.makedirs(os.path.split(cfg.probs_cache_file)[0], exist_ok=True)\n            logging.info(f\"Writing pickle files of probabilities at '{cfg.probs_cache_file}'...\")\n            with open(cfg.probs_cache_file, 'wb') as f_dump:\n                pickle.dump(all_probs, f_dump)\n\n    wer_dist_greedy = 0\n    cer_dist_greedy = 0\n    words_count = 0\n    chars_count = 0\n    for batch_idx, probs in enumerate(all_probs):\n        print(f\"Type of probs: {type(probs)}\")\n        preds = np.argmax(probs, axis=0)\n        preds_tensor = torch.tensor(preds, device='cpu').unsqueeze(0)\n        if isinstance(asr_model, EncDecHybridRNNTCTCModel):\n            pred_text = asr_model.ctc_decoding.ctc_decoder_predictions_tensor(preds_tensor)[0][0]\n        else:\n            pred_text = asr_model.wer.decoding.ctc_decoder_predictions_tensor(preds_tensor)[0][0]\n\n        if cfg.text_processing.do_lowercase:\n            pred_text = punctuation_capitalization.do_lowercase([pred_text])[0]\n        if cfg.text_processing.rm_punctuation:\n            pred_text = punctuation_capitalization.rm_punctuation([pred_text])[0]\n        if cfg.text_processing.separate_punctuation:\n            pred_text = punctuation_capitalization.separate_punctuation([pred_text])[0]\n\n        pred_split_w = pred_text.split()\n        target_split_w = target_transcripts[batch_idx].split()\n        pred_split_c = list(pred_text)\n        target_split_c = list(target_transcripts[batch_idx])\n\n        wer_dist = editdistance.eval(target_split_w, pred_split_w)\n        cer_dist = editdistance.eval(target_split_c, pred_split_c)\n\n        wer_dist_greedy += wer_dist\n        cer_dist_greedy += cer_dist\n        words_count += len(target_split_w)\n        chars_count += len(target_split_c)\n        \n\n    logging.info('Greedy WER/CER = {:.2%}/{:.2%}'.format(wer_dist_greedy / words_count, cer_dist_greedy / chars_count))\n\n    asr_model = asr_model.to('cpu')\n\n    if cfg.decoding_mode == \"beamsearch_ngram\":\n        if not os.path.exists(cfg.kenlm_model_file):\n            raise FileNotFoundError(f\"Could not find the KenLM model file '{cfg.kenlm_model_file}'.\")\n        lm_path = cfg.kenlm_model_file\n    else:\n        lm_path = None\n\n    # 'greedy' decoding_mode would skip the beam search decoding\n    if cfg.decoding_mode in [\"beamsearch_ngram\", \"beamsearch\"]:\n        if cfg.beam_width is None or cfg.beam_alpha is None or cfg.beam_beta is None:\n            raise ValueError(\"beam_width, beam_alpha and beam_beta are needed to perform beam search decoding.\")\n        params = {'beam_width': cfg.beam_width, 'beam_alpha': cfg.beam_alpha, 'beam_beta': cfg.beam_beta}\n        hp_grid = ParameterGrid(params)\n        hp_grid = list(hp_grid)\n\n        best_wer_beam_size, best_cer_beam_size = None, None\n        best_wer_alpha, best_cer_alpha = None, None\n        best_wer_beta, best_cer_beta = None, None\n        best_wer, best_cer = 1e6, 1e6\n\n        logging.info(f\"==============================Starting the beam search decoding===============================\")\n        logging.info(f\"Grid search size: {len(hp_grid)}\")\n        logging.info(f\"It may take some time...\")\n        logging.info(f\"==============================================================================================\")\n\n        if cfg.preds_output_folder and not os.path.exists(cfg.preds_output_folder):\n            os.mkdir(cfg.preds_output_folder)\n        for hp in hp_grid:\n            if cfg.preds_output_folder:\n                preds_output_file = os.path.join(\n                    cfg.preds_output_folder,\n                    f\"preds_out_width{hp['beam_width']}_alpha{hp['beam_alpha']}_beta{hp['beam_beta']}.tsv\",\n                )\n            else:\n                preds_output_file = None\n\n            candidate_wer, candidate_cer = beam_search_eval(\n                asr_model,\n                cfg,\n                all_probs=all_probs,\n                target_transcripts=target_transcripts,\n                preds_output_file=preds_output_file,\n                lm_path=lm_path,\n                beam_width=hp[\"beam_width\"],\n                beam_alpha=hp[\"beam_alpha\"],\n                beam_beta=hp[\"beam_beta\"],\n                beam_batch_size=cfg.beam_batch_size,\n                progress_bar=True,\n                punctuation_capitalization=punctuation_capitalization,\n            )\n\n            if candidate_cer < best_cer:\n                best_cer_beam_size = hp[\"beam_width\"]\n                best_cer_alpha = hp[\"beam_alpha\"]\n                best_cer_beta = hp[\"beam_beta\"]\n                best_cer = candidate_cer\n\n            if candidate_wer < best_wer:\n                best_wer_beam_size = hp[\"beam_width\"]\n                best_wer_alpha = hp[\"beam_alpha\"]\n                best_wer_beta = hp[\"beam_beta\"]\n                best_wer = candidate_wer\n\n        logging.info(\n            f'Best WER Candidate = {best_wer:.2%} :: Beam size = {best_wer_beam_size}, '\n            f'Beam alpha = {best_wer_alpha}, Beam beta = {best_wer_beta}'\n        )\n\n        logging.info(\n            f'Best CER Candidate = {best_cer:.2%} :: Beam size = {best_cer_beam_size}, '\n            f'Beam alpha = {best_cer_alpha}, Beam beta = {best_cer_beta}'\n        )\n        logging.info(f\"=================================================================================\")\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['HYDRA_FULL_ERROR'] = '1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py nemo_model_file=\"/kaggle/working/final_asr_model.nemo\" \\\n       input_manifest=\"/kaggle/input/dataset-ja/banana.json\" \\\n       kenlm_model_file=\"/kaggle/working/kenlm_model.binary\" \\\n       beam_width=\"[128, 256]\" \\\n       beam_alpha=\"[0.5, 1.0]\" \\\n       beam_beta=\"[0.5, 1.0]\" \\\n       preds_output_folder=\"/kaggle/working/predictions\" \\\n       probs_cache_file=null \\\n       decoding_mode=beamsearch_ngram \\\n       decoding_strategy=\"beam\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/NeMo/scripts/asr_language_modeling/ngram_lm/eval_beamsearch_ngram_ctc.py  \\\n       decoding_strategy=\"flashlight\" \\\n        nemo_model_file=\"/kaggle/working/final_asr_model.nemo\"\\\n       input_manifest=\"/kaggle/input/dataset-ja/banana.json\" \\\n       kenlm_model_file=\"/kaggle/working/kenlm_model.binary\" \\\n       beam_width=\"[128, 256]\" \\\n       beam_alpha=\"[0.5, 1.0]\" \\\n       beam_beta=\"[0.5, 1.0]\" \\\n       preds_output_folder=\"/kaggle/working/predictions\" \\\n       probs_cache_file=null \\\n       decoding_mode=beamsearch_ngram \\\n       +decoding.beam.flashlight_cfg.beam_size_token=32 \\\n       +decoding.beam.flashlight_cfg.beam_threshold=25.0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/configs/conformer_ctc_bpe.yaml\nname: \"Conformer-CTC-BPE\"\n\nmodel:\n  sample_rate: 16000\n  log_prediction: true # enables logging sample predictions in the output during training\n  ctc_reduction: 'mean_batch'\n  skip_nan_grad: false\n\n  train_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_train.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: true\n    num_workers: 8\n    pin_memory: true\n    max_duration: 28 # it is set for LibriSpeech, you may need to update it for your dataset\n    min_duration: 0.384\n    # tarred datasets\n    is_tarred: false\n    tarred_audio_filepaths: null\n    shuffle_n: 2048\n    # bucketing params\n    bucketing_strategy: \"synced_randomized\"\n    bucketing_batch_size: null\n\n  validation_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/banana.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  test_ds:\n    manifest_filepath: \"/kaggle/input/dataset-ja/final_test.json\"\n    sample_rate: ${model.sample_rate}\n    batch_size: 16 # you may increase batch_size if your memory allows\n    shuffle: false\n    use_start_end_token: false\n    num_workers: 8\n    pin_memory: true\n\n  # recommend to SPE Unigram tokenizer with small vocab size of 128 or 256 when using 4x sub-sampling\n  # you may find more detail on how to train a tokenizer at: /scripts/tokenizers/process_asr_text_tokenizer.py\n  tokenizer:\n    dir: \"/kaggle/working/SphinxSpeech/tokenizer_spe_unigram_v64\"  # path to directory which contains either tokenizer.model (bpe) or vocab.txt (wpe)\n    type: bpe  # Can be either bpe (SentencePiece tokenizer) or wpe (WordPiece tokenizer)\n\n  preprocessor:\n    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n    sample_rate: ${model.sample_rate}\n    normalize: \"per_feature\"\n    window_size: 0.025\n    window_stride: 0.01\n    window: \"hann\"\n    features: 80\n    n_fft: 512\n    log: true\n    frame_splicing: 1\n    dither: 0.00001\n    pad_to: 0\n    pad_value: 0.0\n\n  spec_augment:\n    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n    freq_masks: 2 # set to zero to disable it\n    # you may use lower time_masks for smaller models to have a faster convergence\n    time_masks: 5 # set to zero to disable it\n    freq_width: 27\n    time_width: 0.05\n\n  encoder:\n    _target_: nemo.collections.asr.modules.ConformerEncoder\n    feat_in: ${model.preprocessor.features}\n    feat_out: -1 # you may set it if you need different output size other than the default d_model\n    n_layers: 16\n    d_model: 176\n\n    # Sub-sampling params\n    subsampling: striding # vggnet, striding, stacking or stacking_norm, dw_striding\n    subsampling_factor: 4 # must be power of 2 for striding and vggnet\n    subsampling_conv_channels: -1 # -1 sets it to d_model\n    causal_downsampling: false\n\n    # Feed forward module's params\n    ff_expansion_factor: 4\n\n    # Multi-headed Attention Module's params\n    self_attention_model: rel_pos # rel_pos or abs_pos\n    n_heads: 4 # may need to be lower for smaller d_models\n    # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention\n    att_context_size: [-1, -1] # -1 means unlimited context\n    att_context_style: regular # regular or chunked_limited\n    xscaling: true # scales up the input embeddings by sqrt(d_model)\n    untie_biases: true # unties the biases of the TransformerXL layers\n    pos_emb_max_len: 5000\n\n    # Convolution module's params\n    conv_kernel_size: 31\n    conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)\n    # conv_context_size can be\"causal\" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size\n    # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]\n    conv_context_size: null\n\n    ### regularization\n    dropout: 0.1 # The dropout used in most of the Conformer Modules\n    dropout_pre_encoder: 0.1 # The dropout used before the encoder\n    dropout_emb: 0.0 # The dropout used for embeddings\n    dropout_att: 0.1 # The dropout for multi-headed attention modules\n\n    # set to non-zero to enable stochastic depth\n    stochastic_depth_drop_prob: 0.0\n    stochastic_depth_mode: linear  # linear or uniform\n    stochastic_depth_start_layer: 1\n\n  decoder:\n    _target_: nemo.collections.asr.modules.ConvASRDecoder\n    feat_in: null\n    num_classes: -1\n    vocabulary: []\n\n  # config for InterCTC loss: https://arxiv.org/abs/2102.03216\n  # specify loss weights and which layers to use for InterCTC\n  # e.g., to reproduce the paper results, set loss_weights: [0.3]\n  # and apply_at_layers: [8] (assuming 18 layers). Note that final\n  # layer loss coefficient is automatically adjusted (to 0.7 in above example)\n  interctc:\n    loss_weights: []\n    apply_at_layers: []\n\n  optim:\n    name: adamw\n    lr: 5.0\n    # optimizer arguments\n    betas: [0.9, 0.98]\n    # less necessity for weight_decay as we already have large augmentations with SpecAug\n    # you may need weight_decay for large models, stable AMP training, small datasets, or when lower augmentations are used\n    # weight decay of 0.0 with lr of 2.0 also works fine\n    weight_decay: 1e-3\n\n    # scheduler setup\n    sched:\n      name: NoamAnnealing\n      d_model: ${model.encoder.d_model}\n      # scheduler config override\n      warmup_steps: 10000\n      warmup_ratio: null\n      min_lr: 1e-6\n\ntrainer:\n  devices: -1 # number of GPUs, -1 would use all available GPUs\n  num_nodes: 1\n  max_epochs: 3\n  max_steps: -1 # computed at runtime if not set\n  val_check_interval: 1.0 # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n  accelerator: auto\n  strategy: ddp\n  accumulate_grad_batches: 1\n  gradient_clip_val: 0.0\n  precision: 32  # 16, 32, or bf16\n  log_every_n_steps: 10  # Interval of logging.\n  enable_progress_bar: True\n  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it\n  check_val_every_n_epoch: 1 # number of evaluations on validation every n epochs\n  sync_batchnorm: true\n  enable_checkpointing: False  # Provided by exp_manager\n  logger: false  # Provided by exp_manager\n  benchmark: false # needs to be false for models with variable-length speech input as it slows down training\n\nexp_manager:\n  exp_dir: \"/kaggle/working/results\"\n  name: ${name}\n  create_tensorboard_logger: true\n  create_checkpoint_callback: true\n  checkpoint_callback_params:\n    # in case of multiple validation sets, first one is used\n    monitor: \"val_wer\"\n    mode: \"min\"\n    save_top_k: 5\n    always_save_nemo: True # saves the checkpoints as nemo files instead of PTL checkpoints\n\n  # you need to set these two to True to continue the training\n  resume_if_exists: false\n  resume_ignore_no_checkpoint: false\n\n  # You may use this section to create a W&B logger\n  create_wandb_logger: false\n  wandb_logger_kwargs:\n    name: null\n    project: null\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/ASR-Squad/Tokenizers/tokenizer_spe_unigram_v64/vocab.txt\n\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##>\n##l\n##f\n##i\n##g\n##h\n##u\n##e\n##p\n##r\n##v\n##o\n##\n##\n##?\n##\n##\n##a\n##<\n##=\n##[\n##n\n##\n##\n##\n##\n##\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/ASR-Squad/Tokenizers/tokenizer_spe_unigram_v64/tokenizer.vocab\n<unk>\t0\n\t-2.2088\n\t-2.40748\n\t-2.65333\n\t-2.82795\n\t-2.85866\n\t-2.87771\n\t-3.146\n\t-3.22545\n\t-3.30548\n\t-3.33668\n\t-3.46445\n\t-3.50065\n\t-3.65437\n\t-3.78068\n\t-3.88658\n\t-3.91189\n\t-4.06987\n\t-4.09701\n\t-4.19604\n\t-4.32791\n\t-4.70701\n\t-4.83043\n\t-4.86126\n\t-4.89308\n\t-5.05488\n\t-5.18295\n\t-5.67968\n\t-5.85408\n\t-5.98021\n\t-6.06764\n\t-6.09118\n\t-6.15143\n\t-6.39132\n\t-6.44179\n\t-6.5542\n\t-7.47499\n>\t-8.40933\nl\t-8.41086\nf\t-8.54166\ni\t-8.54166\ng\t-11.166\nh\t-11.166\nu\t-11.166\ne\t-11.2673\np\t-11.2673\nr\t-11.2673\nv\t-11.2673\no\t-11.2673\n\t-13.9689\n\t-13.9689\n?\t-14.4689\n\t-14.4689\n\t-14.4689\na\t-15.4687\n<\t-15.4688\n=\t-15.4689\n[\t-15.4689\nn\t-15.4689\n\t-15.4689\n\t-15.4689\n\t-15.4689\n\t-15.4689\n\t-15.4689\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile speech_to_text_ctc_bpe.py\n\n\nimport pytorch_lightning as pl\nfrom omegaconf import OmegaConf\n\nfrom nemo.collections.asr.models.ctc_bpe_models import EncDecCTCModelBPE\nfrom nemo.core.config import hydra_runner\nfrom nemo.utils import logging\nfrom nemo.utils.exp_manager import exp_manager\n\n\n@hydra_runner(config_path=\"/kaggle/working/configs/\", config_name=\"conformer_ctc_bpe\")\ndef main(cfg):\n    logging.info(f'Hydra config: {OmegaConf.to_yaml(cfg)}')\n\n    trainer = pl.Trainer(**cfg.trainer)\n    exp_manager(trainer, cfg.get(\"exp_manager\", None))\n    asr_model = EncDecCTCModelBPE(cfg=cfg.model, trainer=trainer)\n\n    # Initialize the weights of the model from another model, if provided via config\n    asr_model.maybe_init_from_pretrained_checkpoint(cfg)\n\n    trainer.fit(asr_model)\n\n    if hasattr(cfg.model, 'test_ds') and cfg.model.test_ds.manifest_filepath is not None:\n        if asr_model.prepare_test(trainer):\n            trainer.test(asr_model)  \n    # Save the model\n    final_model_path = '/kaggle/working/final_asr_model2.nemo'\n    asr_model.save_to(final_model_path)\n    logging.info(f'Model saved at {final_model_path}')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/speech_to_text_ctc_bpe.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/configs/transcribe_speech.py \\\n  model_path=\"/kaggle/working/final_asr_model2.nemo\" \\\n  dataset_manifest=\"/kaggle/input/dataset-ja/final_test.json\" \\\n  output_filename=\"/kaggle/working/test_with_predictions2.json\" \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate WER\n!python /kaggle/working/configs/speech_to_text_eval.py \\\n  dataset_manifest=\"/kaggle/working/test_with_predictions2.json\" \\\n  use_cer=False \\\n  only_score_manifest=True\n\n# Calculate CER\n!python /kaggle/working/configs/speech_to_text_eval.py \\\n  dataset_manifest=\"/kaggle/working/test_with_predictions2.json\" \\\n  use_cer=True \\\n  only_score_manifest=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install botocore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -P configs/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/asr/speech_to_text_eval.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install boto3 --upgrade\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade nemo_toolkit[all]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade botocore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import inspect\nfrom nemo.collections.asr.models import aed_multitask_models\n\nprint(inspect.getmembers(aed_multitask_models, inspect.isfunction))\nprint(inspect.getmembers(aed_multitask_models, inspect.isclass))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\n\n\nworking_dir = \"/kaggle/working\"\n\n\nzip_path = \"/kaggle/working/working_directories.zip\"\n\n\nwith zipfile.ZipFile(zip_path, 'w') as zipf:\n    for root, dirs, files in os.walk(working_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zipf.write(file_path, os.path.relpath(file_path, working_dir))\n\nprint(f\"All directories zipped and saved to {zip_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade nemo-toolkit[all]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load /kaggle/working/test_with_predictions.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/Alkholy53/ASR-Squad.git\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/ASR-Squad","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch test.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git config credential.helper store\n!git config --global user.email \"a.alkholy53@student.aast.edu\"\n!git config --global user.name \"Alkholy53\"\n!git add .\n!git commit -m \"Add file from Kaggle\"\n!git push origin main\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'test.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!touch test.nemo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n\n!tar -czf Landscapess.tar.gz ASR-Squad\n\nfrom IPython.display import FileLink\n\nFileLink(r'Landscapess.tar.gz')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set your own project id here\nPROJECT_ID = 'your-google-cloud-project'\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push https://github.com/Alkholy53/ASR-Squad.git main","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set Git configurations (optional if not already set globally)\nos.system('git config --global user.email \"a.alkholy53@student.aast.edu\"')\nos.system('git config --global user.name \"Alkholy53\"')\n# Add all files to the Git staging area\nos.system('git add .')\n\n# Commit the changes with a commit message\ncommit_message = \"Add file from Kaggle\"\nos.system(f'git commit -m \"{commit_message}\"')\n\n# Push changes to the main branch of the remote repository (origin)\nos.system('git push origin main')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Add all files to the Git staging area\nos.system('git add .')\n\n# Commit the changes with a commit message\ncommit_message = \"Add file from Kaggle\"\nos.system(f'git commit -m \"{commit_message}\"')\n\n# Push changes to the main branch of the remote repository (origin)\nos.system('git push origin main')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ssh-keygen -t rsa -b 4096 -C \"a.alkholy53@student.aast.edu\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\n\nSCOPES = ['https://www.googleapis.com/auth/drive.file']\n\nflow = InstalledAppFlow.from_client_secrets_file(\n    'credentials.json', SCOPES)\ncredentials = flow.run_local_server(port=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.oauth2.credentials import Credentials\nfrom google.auth.transport.requests import Request\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\n\nSCOPES = ['https://www.googleapis.com/auth/drive.file']\n\ndef upload_to_drive(file_path, file_name):\n    creds = None\n    # Load credentials from token.json or credentials.json\n    creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    \n    # If credentials are expired, refresh them\n    if creds and creds.expired and creds.refresh_token:\n        creds.refresh(Request())\n    \n    # Build Google Drive service\n    service = build('drive', 'v3', credentials=creds)\n    \n    # File to be uploaded\n    file_metadata = {'name': file_name}\n    media = MediaFileUpload(file_path, resumable=True)\n    \n    # Upload file to Google Drive\n    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n    \n    print(f'File ID: {file.get(\"id\")}')\n\n# Example usage\nif __name__ == '__main__':\n    upload_to_drive('path_to_your_output_file.csv', 'output_file.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}